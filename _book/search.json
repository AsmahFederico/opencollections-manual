[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenCollections Manual",
    "section": "",
    "text": "Introduction\nReprex’s new OpenCollections system wants to help small and large enterprises work with big data without huge investments into data infrastructure. OpenCollections is a collaborative tool that enables owners of small, local databases to remain competitive in training AI in the age of big data. It helps to fill your databases with up-to-date information, find and correct errors in your databases, and connect your database entries to new information as you need them, without further IT and data investments.\nOpenCollections aims to link various open, public, and private databases and let the knowledge flow among them. OpenCollections is not a database system but a knowledge system that helps to fill up databases with the latest information, find errors in private databases, or support the human-in-the-loop process of overseeing what AI systems are doing on your behalf or perhaps against you.\nThe Open Music Observatory and its music data-sharing space utilised Reprex’s OpenCollections system for data coordination, terminology negotiation, and import.\nLike many applications in the European open data field, OpenCollections is built around Wikibase. This open-source software system has built one of the world’s most extensive knowledge graphs and knowledge bases, Wikidata, which synchronises the knowledge base of the 329 versions of Wikipedia.\nWikibase has been successfully used in many EU projects, including the creation of the EU Knowledge Graph↗ (see: 4.4 The EU Knowledge Graph, (Diefenbach, Wilde, and Alipio 2021)). It also has training material on the EU Academy. While Wikibase is fully open-source and accessible, it is a complicated system that requires many extensions and adoptions to support a data-sharing space or a public-private knowledge base like ours. Reprex’s extensions aim to make data importing and enrichment easier and less costly and make data export more reusable. The Open Music Observatory is not dependent on Reprex’s extensions; however, our dataspace’s automated and semi-automated data-sharing workflows use OpenCollections to speed up the process or the unit-testing of the imports.\nUsing Wikibase allows coordination with Wikidata, which evolved into a central hub on the web of data and it is one of the largest existing knowledge graphs, and perhaps the best known open one. It is synchronised with knowledge from respected public institutions like Eurostat, the German National Library or BBC, and it is one of the backbones of many web services like Google Search. Wikibase is scalable to very big graphs, and it has many thousand users with a simple and intuitive user interface.\nWe are augmenting Wikibase with further software components developed by Reprex that allows its smooth operation in various institutional and enterprise settings. The choice of Wikibase as a core component allows an easy onboarding; furthermore, knowledge coordination teams can easily find new colleagues, tutorials, and help with the globally known Wikidata/Wikibase interface. See 3.1.1 Getting started with Wikidata to get started. The creation of OpenCollections accounts is explained step-by-step in 5.1 Create an Account.\nWhile OpenCollections is almost exclusively built from open-source components and taps into various open knowledge systems, it was not designed to work only with open data; in fact, its primary use is to improve the quality of proprietary, often very confidential, and protected databases. Such databases usually need authoritative data from statistical agencies, scientific bodies, or governmental registries; connecting your systems with open knowledge graphs can ensure that such data is imported without delay and error into your systems. Similarly, you may need to inform the world about your copyright-protected repertoire or the technical details of your product offering. Placing the information on global knowledge graphs ensures that thousands of web services will effortlessly use the updated, new, or correct information about your offering.\n\n\n\n\nDiefenbach, Dennis, Max de Wilde, and Samantha Alipio. 2021. “Wikibase as an Infrastructure for Knowledge Graphs: The EU Knowledge Graph.” In ISWC 2021. Online, France. https://hal.science/hal-03353225.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "inspiration.html",
    "href": "inspiration.html",
    "title": "1  Inspiration",
    "section": "",
    "text": "1.1 We need data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inspiration</span>"
    ]
  },
  {
    "objectID": "inspiration.html#sec-inspiration-data-need",
    "href": "inspiration.html#sec-inspiration-data-need",
    "title": "1  Inspiration",
    "section": "",
    "text": "1.1.1 No Data is Available: This Scientist Stung Himself With Dozens Of Insects Because No One Else Would\n\n\n\nGood data curators are people who share a passion for measuring, recording and categorising the knowledge about their field, be it insects, music, or informal economy.\n\n\nThe Schmidt Pain Index, as its informally known, runs from 1-4. The common honey bee serves as its anchor point, a solid 2. At the top end of the scale lie the bullet ant and the tarantula hawk (which is neither a tarantula nor a hawk; it’s a wasp). Watch the video with Dr. Schmidt, and listen to the whole interview here. ⏯ This Scientist Stung Himself With Dozens Of Insects Because No One Else Would.\n\n\n1.1.2 Nobody Counted Them Before: Big Data Is Saving This Little Bird\n“We need to improve conservation by improving wildlife monitoring. Counting plants and animals is really tricky business.” ⏯ Big Data Is Saving This Little Bird",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inspiration</span>"
    ]
  },
  {
    "objectID": "inspiration.html#sec-web-30",
    "href": "inspiration.html#sec-web-30",
    "title": "1  Inspiration",
    "section": "1.2 From Datasets and Files to Living Web Resoures",
    "text": "1.2 From Datasets and Files to Living Web Resoures\n\n1.2.1 Web 3.0\n\n\n\nWe are structuring your knowledge in a way that it results in datasets that can be connected similarly to the World Wide Web pages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inspiration</span>"
    ]
  },
  {
    "objectID": "inspiration.html#critical-attitude",
    "href": "inspiration.html#critical-attitude",
    "title": "1  Inspiration",
    "section": "1.3 Remain Critical: Ethical Data, Trustworthy AI",
    "text": "1.3 Remain Critical: Ethical Data, Trustworthy AI\nSometimes we put our hands on data that looks like a unique starting point to create a new indicator. But our indicator will be flawed if the original dataset is flawed. And it can be flawed in many ways, most likely that some important aspect of the information was omitted, or the data is autoselected, for example, under-sampling women, people of colour, or observations from small or less developed countries.\n\n1.3.1 Machine Learning from Bad Data: Weapons of Math Destruction, Algorithms of Oppression\nCathy O’Neil: ⏯ Weapons of math destruction, which O’Neil are mathematical models or algorithms that claim to quantify important traits: teacher quality, recidivism risk, creditworthiness but have harmful outcomes and often reinforce inequality, keeping the poor poor and the rich rich. They have three things in common: opacity, scale, and damage. https://blogs.scientificamerican.com/roots-of-unity/review-weapons-of-math-destruction/](https://blogs.scientificamerican.com/roots-of-unity/review-weapons-of-math-destruction/)\nIn ⏯ Algorithms of Oppression, Safiya Umoja Noble challenges the idea that search engines like Google offer an equal playing field for all forms of ideas, identities, and activities. Data discrimination is a real social problem; Noble argues that the combination of private interests in promoting certain sites, along with the monopoly status of a relatively small number of Internet search engines, leads to a biased set of search algorithms that privilege whiteness and discriminate against people of colour, especially women of colour.\n\n\n1.3.2 Big Data Creates Inequalities: Data Feminism\nCatherine D’Ignazio and Lauren F. Klein: ⏯ Data Feminism. This is a much-celebrated book and with a good reason. It views AI and data problems from a feminist point of view, but the examples and the toolbox can be easily imagined for small-country biases, racial, ethnic, or small enterprise problems. A very good introduction to the injustice of big data and the fight for a fairer use of data, and how bad data collection practices through garbage in-garbage out lead to misleading information or even misinformation.\n\n\n1.3.3 Bad Data collection Used for Modeling: Why The Bronx Burned\nWhy The Bronx Burned. Between 1970 and 1980, seven census tracts in the Bronx lost more than 97 percent of their buildings to fire and abandonment. In his book ⏯ The Fires, Joe Flood blames the misguided “best and brightest” effort by New York City to increase government efficiency. With the help of the Rand Corp., the city tried to measure fire response times, identify redundancies in service, and close or re-allocate fire stations accordingly. What resulted, though, was a perfect storm of bad data: The methodology was flawed, the analysis was rife with biases, and the results were interpreted in a way that stacked the deck against poorer neighbourhoods. The slower response times allowed smaller fires to rage uncontrolled in the city’s most vulnerable communities. Listen to the podcast here.\n\n\n1.3.4 Bad Incentives Are Blocking Better Science\nBad Incentives Are Blocking Better Science “There’s a difference between an answer and a result. But all the incentives are pointing toward telling you that as soon as you get a result, you stop.” After the deluge of retractions, the stories of fraudsters, the false positives, and the high-profile failures to replicate landmark studies, some people have begun to ask: “⏯ Is science broken?”. Listen to the pdocast [⏯Science is Hard]tttps://podcasts.apple.com/us/podcast/10-science-is-hard/id1011406983?i=1000391467935)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inspiration</span>"
    ]
  },
  {
    "objectID": "inspiration.html#reality-check",
    "href": "inspiration.html#reality-check",
    "title": "1  Inspiration",
    "section": "1.4 Reality Check",
    "text": "1.4 Reality Check\n\n1.4.1 Looking Behind Data: Moving to America’s Worst Place to Live\nChristopher Ingraham wrote ⏯ a quick blog post for The Washington Post about an obscure USDA data set called the natural amenities index, which attempts to quantify the natural beauty of different parts of the country. He described the rankings, noted the counties at the top and bottom, hit publish and did not think much of it. Almost immediately, he started to hear from the residents of northern Minnesota, who were not very happy that Chris had written, “The absolute worst place to live in America is (drumroll, please) … Red Lake County, Minn.” He could not have been more wrong … a year later he moved to Red Lake County with his family.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inspiration</span>"
    ]
  },
  {
    "objectID": "tidy.html",
    "href": "tidy.html",
    "title": "2  Tidy work",
    "section": "",
    "text": "2.1 Tidy data\nOur reproducible research practice follows the tidy data principle, which has very complex computer science and information management consequences. Still, for the lay user of data, it boils down to simplicity.\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\nIn tidy data:\nThis is often far more easier to write than to do, but still, if you can make it that simple, then you already mastered Codd’s 3rd normal form framed in statistical language.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy work</span>"
    ]
  },
  {
    "objectID": "tidy.html#sec-tidy-data",
    "href": "tidy.html#sec-tidy-data",
    "title": "2  Tidy work",
    "section": "",
    "text": "Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. From R For Data Science - 12. Tidy Data\n\n\n\n\nEvery column is a variable. We do not use colours (our machine-to-machine pipelines is colourblind). If we need comments or specifications, we add a new column.\nEvery row is an observation. Every variable belonging to Bulgaria is in the Bulgaria row, and there is one and only Bulgaria row.\nEvery cell is a single value. We never merge cells! A tidy dataset has no divided columns and no divided rows.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy work</span>"
    ]
  },
  {
    "objectID": "tidy.html#sec-markup-text",
    "href": "tidy.html#sec-markup-text",
    "title": "2  Tidy work",
    "section": "2.2 Markup text",
    "text": "2.2 Markup text\nWe create interconnected, interoperable (web) resources. We want to ensure that our research results are findable, accessible, and reusable. The World Wide Web has been a source of high interoperability and findability in the last 30 years, with the introduction of the HTTP protocol and the standardization of the HTML text markup language.\nAll our output needs to be converted to HTML, but that does not mean we need to work in an HTML editor. However, the need for interoperability among operating systems (Windows, MacOs, Linux) and software packages (at least from Word, Libre, and Google Docs to HTML, preferably to PDF, too) requires a simple, common notation.\nMarkdown is a simplified HTML text notation that works well with word processors.\n\n\n\n\n\nIf you want Word output, Word is rendered instead of HTML. You can also create a PDF or EPUB and even a PPTX output.\n\n2.2.1 Markdown editors\nThere are countless Markdown editors. Because Markdown is so simple, you can, if you want to, edit markdown files in Notepad, WordPad (Windows) or VIM (Linux).\nMost word processors support markdown. For example, Google Docs has a free extension that converts and document from Docs to markdown.\n\n\n\nDilinger is one of the best editors, and it is particularly suitable foor first-time markup users, as you immediately get visual feedback on how you mark up your text.\n\n\nThere are several online Markdown editors that you can use to try writing in Markdown. Dillinger is one of the best online Markdown editors. Just open the site and start typing in the left pane. A preview of the rendered document appears in the right pane.\n\nBasic Syntax\nExtended Syntax\n\n\n\n2.2.2 Wikipedia & MediaWiki\nThe documentation of our knowledge base and terminological agreements is documented in MediaWiki, the software that makes Wikipedia editable, too. It uses a form of markdown for an interoperable and simple editing of interlinked documents, images, and data documents.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tidy work</span>"
    ]
  },
  {
    "objectID": "wikidata.html",
    "href": "wikidata.html",
    "title": "3  Wikidata and Other Open Knowledge Graphs",
    "section": "",
    "text": "3.1 Connect to Wikidata\nWikidata is a collaboratively edited multilingual knowledge graph hosted by the Wikimedia Foundation. It is a common source of open data that Wikimedia projects, such as Wikipedia, and anyone else, is able to use under the CC0 public domain license. As of early 2023, Wikidata had 1.54 billion item statements, or small, verifiable, scientific statements about our world.\nWikidata is a document-oriented database, focusing on items, which represent any kind of topic, concept, or object.\nKnowledge graphs connect things in the real world, because their nodes—in Wikidata, the conceptual document—, represent people, objects, and their relationships as they are out there, and not as they are represented by an “ordinary” database . The Q42 document about the late English writer and humorist Douglas Adams connects facts about his life (birthday, place of birth, time of death), and connects him to his books, their translations, identifiers to look up these books, and so on.\nWikidata is a knowledge graph: it connects the concept of Douglas Adams (Q42), to the concept of his most quoted humorous episode from his world-famous Hitchhiker’s Guide to the Galaxy (Q25169) , which is a similarly structured document about the five books of his series, which document is further connected in the graph to the concept of the books’ Serbian translation (Q117279887).\nWikidata is not a database but a very useful system for filling up and keeping many databases in sync worldwide. If your own institutional or private library has a catalogue, you may have a copy of the Hitchhiker’s Guide to the Galaxy; in this case, your catalogue is likely to have a local, private identifier to your copy of the book. Imagine your little private catalogue, where you, like the editors of Wikidata, reserved the #42 entry to Douglas Adams’ book.\n|——-|—————————————————————-|————————————————————————————-| | My-01 | Martell, Yann (Q13914) | Life of Pi (Q374204) | | … | … | … | | My-42 | Adams, Douglas (Q42) | Hitchhiker’s Guide to the Galaxy (Q25169) | | … | … | … |\nIf you can connect your My-42 entry of Hitchhiker’s Guide to the Galaxy with the books’ Wikidata entry Q25169, you can import a wealth of information into your private catalogue. Furthermore, if you connect the Wikidata item Q42 of the author Douglas Adams to your catalogue’s own entry about the author, you can import a lot of additional knowledge, for example, information about his other works, or the end term of these books’ copyright protection, after which they will become public domain and they will be free to copy and distribute.\nIn Wikidata, each item has a unique, persistent identifier, a positive integer number, prefixed with the upper-case letter Q, known as a “QID”.Global information systems like to anchor authoritative information about people, books, musical works, and other important things to persistent identifiers. For example, in VIAF, the authority file that keeps information synchronised across national libraries, Douglas Adams’ persistent identifier is 113230702, whereas in the Portugese National Library it is 68537. Wikidata is particularly useful because it serves as an “identity broker”, and this linking information can be retrieved directly from Douglas Adams’ Q42 page.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wikidata and Other Open Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikidata.html#sec-wikidata",
    "href": "wikidata.html#sec-wikidata",
    "title": "3  Wikidata and Other Open Knowledge Graphs",
    "section": "",
    "text": "Wikidata is a document-oriented database. This document connects a lot of knowledge about the late English writer and humorist, Douglas Adams.\n\n\n\n\n\nID | Author | Title |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.1 Getting started with Wikidata\n\n3.1.1.1 Global Identities\nMr and Mrs Barasits, a.k.a. János Barasits (1859-1935) and his wife, Barasits, Jánosné, born Pichler, Kornélia, were prominent postcard producers and publishers at the beginning of the 20th century. They produced plenty of beautiful postcards.\nIn the 1920s and 1930s, the authors’ right (~copyright) protection of photographs and postcards was relatively short, only 15 years, so their postcards went into the public domain in terms of copying long ago. Plenty of their beautiful works is out there on the internet, but it is very hard to put them into a collection, because most databases know next to nothing about the identities of these creators and their creations.\nUnfortunately, you cannot find their name in the most commonly used authority controls, i.e., VIAF or ISNI. Writing to VIAF is only possible via member institutions, and ISNI costs money. A temporary solution is to create a Wikidata QID for János Barasits (Q124423018), until somebody registers his name into VIAF. With this entry, it will be easier to find further postcards from them, or other information about them all over the world!\nWriting in Wikidata is free for all and subject to community review. If you read this tutorial, please pledge to record new persons (or other items) into Wikidata, only if your knowledge is solid. You can verify the information needed through proper research.\n\n\n3.1.1.2 Create a Wikidata Item\nIn this tutorial, you can learn how to create a new item on Wikidata. Countless web and AI applications and millions of people use Wikidata, so in the beginning it is recommended to not experiment with it in the live system. Wikidata has a Sandbox for practising. We recommend using it as a first step. If you work with Wikibase, particularly with Reprex’s OpenCollections, you will have access to a similar sandbox. It will be prefilled with data, concepts, and properties suitable for your learning needs, often going beyond what you would find in the public Wikidata.\nLet’s see how you can create your own János Barasits item. \n\nYou can see how creating a new item looks like in the system:\n\n\n\n\n\nThe first step in creating an item (in this case an item for János Barasits) is providing the two most important information for an item, which is the Label and the Description.\nThe Label is the name of the item (in our case the label of the item will be “János Barasits”).\nThe Description contains a short explanation of our item (in our case the description for the item will be “Hungarian postcard maker and publisher).\nAliases are alternative names for the item.\n\nAfter creating the item with the basic information of Label and Description we can weave this information entry into the knowledge graph. At this point, János Barasits could be a person, it could be a book titled after the person, or a photo of the person. Connecting János Barasits to other entities, such as the concept of a human being, will allow other people and their computer systems to understand that we are talking about a person here.You can do that by creating “Statements”. The property “instance of” defines the class our item is a particular example or member of. In this case we would like to make a statement about our item “János Barasits” defining with the property “instance of” that he is a member of the “human” class.\n\n\n\n\n\nThrough the sandbox explore the different type of properties and statements. Add a few basic statement to your new item:\n\nJános Barasits is a human—his gender was male—he was born in 1859 (with the precision of a year only)—he died in 1935.\n\nIt should look similar to this:\n\n\n\n\n\nTo see another example on how useful knowledge graphs can be consider the following. The four character, 1935 can be understood as a number for most readers, but such a data point without a defined meaning is useless. In a basic database you would see 1935 and know that it is a number. However, in knowledge graphs, like here on Wikidata, when we add the “metadata”, and we connect 1935 to the definition of date of death, we add a meaning (“semantics”) to the number 1935. Now, 1935 is not only a number but also a date of someone’s death.\nThe definition of date of death is useful in itself, but in a knowledge base, we can do even more with this piece of information. With this information we can combine the fact that in Europe the copyright protection term of people’s creation runs up to 70 years after their death. Thus, a knowledge base can infere the fact that currently János Barasits’s postcards are out of copyright and they can be freely copied and distributed!\nHere is a very basic Wikidata page for János Barasits. What is very important, is that we have a globally unique identifier, Q124423018 that uniquely identifies him as a human. If you have a collection of postcards (digitals or analogue, vintage physical objects), connecting your own database with Q124423018 will help you to import the knowledge of the expired copyright protection term; it will help you finding other out-of-copyright scanned copies of Barasits’ postcards; it will be easier to connect to other collections that hold items from them, and so on.\n\n\n\n3.1.2 Retrieve an item from Wikidata\nMany internal enterprise resource systems or APIs use SQL, a 50+ year-old data query language. SQL is the lingua franca of relational database systems; you may be familiar with it. Can you query Wikidata in SQL?\nNot exactly. It requires a different querying language, which was developed for knowledge graphs. It is called SPARQL because it is similar to SQL, but they are rather distant cousins.\nWhile SQL is widely used, it does have a significant limitation: your query scripts are specific to one database system or API. What works in your internal catalogue may not function in another organization’s. If you’ve written a script to update your data from a specific web API, it doesn’t guarantee that the script will be compatible with another API. Furthermore, it’s not future-proof: if the API owner (or your database manager) makes even a slight adjustment to the system, you may need to modify or rewrite your retrieval code.\nRemember, the significant advantage of Wikidata and other open knowledge graphs is that millions of people work on improvements and extensions daily. This means that an SQL request would be outdated every day. Instead of SQL, SPARQL queries do not look for cells in data tables, but they use intelligent knowledge to find the cells containing data about what you need. In SQL, you need to know which table contains people’s birthdays and death dates to find out the year when János Barasits died. In SPARQL, you are looking for the cell that contains the date of death for the human known as János Barasits.\n\n\n3.1.3 SPARQL basics\nSPARQL is the standard query language and protocol for Linked Open Data and RDF databases. Having been designed to query a great variety of data, it can efficiently extract information hidden in non-uniform data and store it in various formats and sources. SPARQL, pronounced ‘sparkle’, is the standard query language and protocol for Linked Open Data on the web or RDF triplestores. The SPARQL standard is designed and endorsed by the World Wide Web Consortium and helps users and developers focus on what they would like to know instead of how a database is organised.\nOur data curators must be able to run SPARQL queries and make elementary modifications to them. Because we often import very large datasets, it would be very difficult to manually control every record on the graphical user interface. We use pre-written SPARQL queries (the data curator is expected to run via a simple URL link, perhaps modifying a class’s QID or a property’s PID) that serve as so-called unit tests. These queries programmed by Reprex allow simple tests like these:\n\nIf the curator gave us 5432 person records, we have 5432 persons in the Reprexbase instance;\nIf the gender breakup of a person’s records is 2834:2598, the instance results in exactly the same persons of two genders (assuming that no third gender is used in the original data.)\nIf we received data on Ján Levoslav Bella’s Symphony in B minor, the publication year is 1982.\n\nA simple SPARQL query looks like this:\n\nSELECT ?a ?b ?c\nWHERE\n{\n  x y ?a.\n  m n ?b.\n  ?b f ?c.\n}\n\nSuppose we want to list all children of the baroque composer Johann Sebastian Bach. Using pseudo-elements like in the queries above, how would you write that query?\nHopefully you got something like this:\n\nSELECT ?child\nWHERE\n{\n  #  child \"has parent\" Bach\n  ?child parent Bach.\n  # (note: everything after a ‘#’ is a comment and ignored by WDQS.)\n}\n\nor this,\n\nSELECT ?child\nWHERE\n{\n  # child \"has father\" Bach \n  ?child father Bach. \n}\n\nor this,\n\nSELECT ?child\nWHERE\n{\n  #  Bach \"has child\" child\n  Bach child ?child.\n}\n\nThe first two triples say that the ?child must have the parent/father Bach; the third says that Bach must have the child ?child. Let’s go with the second one for now.\nSo what remains to be done in order to turn this into a proper WDQS query? On Wikidata, items and properties are not identified by human-readable names like “father” (property) or “Bach” (item). (For good reason: “Johann Sebastian Bach” is also the name of a German painter, and “Bach” might also refer to the surname, the French commune, the Mercury crater, etc.) Instead, Wikidata items and properties are assigned an identifier. To find the identifier for an item, we search for the item and copy the Q-number of the result that sounds like it’s the item we’re looking for (based on the description, for example). To find the identifier for a property, we do the same, but search for “P:search term” instead of just “search term”, which limits the search to properties. This tells us that the famous composer Johann Sebastian Bach is Q1339, and the property to designate an item’s father is P:P22.\nAnd last but not least, we need to include prefixes. For simple WDQS triples, items should be prefixed with wd:, and properties with wdt:. (But this only applies to fixed values – variables don’t get a prefix!)\nPutting this together, we arrive at our first proper WDQS query:\n\nSELECT ?child\nWHERE\n{\n# ?child  father   Bach\n  ?child wdt:P22 wd:Q1339.\n}\n\nTry the first query:\n\n\n\nClick on the image to try it out live on the Wikidata SPARQL Endpoint. The query will run if you press the ▶ sign on the endpoint in the bottom left corner.\n\n\nThe first querry will provide you with identifiers, which is great if you are a programmer and you are wiring your database to Wikidata, but less impressive if you are getting familiar with SPARQL and you want to see clearly the fruits of your work.\nLuckily, Wikidata has a human-friendly extension to SPARQL. If you add the following command to your query: SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\".somewhere within the WHERE clause, you get additional variables: For every variable ?foo in your query, you now also have a variable ?fooLabel, which contains the label of the item behind ?foo.\nIf you add this to the SELECT clause, you get the item as well as its label:\n\nSELECT ?child ?childLabel\nWHERE\n{\n# ?child  father   Bach\n  ?child wdt:P22 wd:Q1339.\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n}\n\n\n\n\nClick on the image to try it out live on the Wikidata SPARQL Endpoint. The query will run if you press the ▶ sign on the endpoint in the bottom left corner↗.\n\n\nTry running that query – you should see not only the item numbers, but also the names of the various children.\n\n\n\nchild\nchildLabel\n\n\n\n\nwd:Q57225\nJohann Christoph Friedrich Bach\n\n\nwd:Q76428\nCarl Philipp Emanuel Bach\n\n\n…\n\n\n\n\n\n\n3.1.4 Pre-filter Wikidata\nWhen you work with OpenCollections or Wikibase, you may want to synchronize your knowledge graph with Wikidata. A straightforward way to do this is to import a part of the Wikidata knowledge graph into your instance.\nImagine you would like to copy the definition of a human, Béla Bartók, to your Wikibase instances. The following querry\n\nSELECT DISTINCT ?itemLabel ?itemLabelLang ?itemDescription ?itemDescriptionLang ?aliases ?aliasesLang WHERE {\n  wd:Q83326 rdfs:label ?itemLabel ;\n            schema:description ?itemDescription .\n  OPTIONAL {\n    wd:Q83326 skos:altLabel ?aliases .\n    BIND(LANG(?aliases) AS ?aliasesLang)\n  }\n  BIND(LANG(?itemLabel) AS ?itemLabelLang)\n  BIND(LANG(?itemDescription) AS ?itemDescriptionLang)\n  FILTER(?itemLabelLang IN (\"en\", \"de\", \"hu\", \"sk\", \"lt\", \"bg\"))\n  FILTER(?itemDescriptionLang IN (\"en\", \"de\", \"hu\", \"sk\", \"lt\", \"bg\"))\n  FILTER(?aliasesLang IN (\"en\", \"de\", \"hu\", \"sk\", \"lt\", \"bg\"))\n}\n\nTry it out↗\n\nYou can modify the querry. In Line 3, the wd:Q83326 identifies the QID for Béla Bartók. Try it out with wd:``28104185!\nWe asked the labelling in six languages. You can use IN (\"en\", \"de\") or even IN (\"de\") if you want to reduce the number of languages or change the language codes.\n\nYou would like to copy property definitions to your Wikibase instance. The following code will provide you the necessary information (without additional statements) about the property wd:P31—a very important property for data modelling.\n\nSELECT ?property ?propertyLabel ?dataType ?propertyDescription ?lang ?alias WHERE {\n  VALUES ?property { wd:P31 }  # Replace these IDs with the property IDs you are interested in\n  ?property a wikibase:Property .\n  ?property wikibase:propertyType ?dataType .\n\n  # Fetch labels in the specified languages\n  ?property rdfs:label ?propertyLabel .\n  BIND(LANG(?propertyLabel) AS ?lang)\n  FILTER(?lang IN (\"en\", \"fr\", \"sk\", \"hu\", \"bg\", \"lt\"))  # Replace these your languages\n  BIND(IF(?lang = \"en\", 1, 2) AS ?labelRank)\n\n  # Fetch descriptions in the specified languages\n  OPTIONAL {\n    ?property schema:description ?propertyDescription .\n    FILTER(LANG(?propertyDescription) IN (\"en\", \"fr\", \"sk\", \"hu\", \"bg\", \"lt\"))\n    FILTER(LANG(?propertyDescription) = ?lang)  # Ensure matching languages\n  }\n  # Fetch aliases in the specified languages\n  OPTIONAL {\n  ?property skos:altLabel ?alias .\n  FILTER(LANG(?alias) IN (\"en\", \"fr\", \"sk\", \"hu\", \"bg\", \"lt\"))\n  FILTER(LANG(?alias) = ?lang)  # Ensure matching languages\n  }\n\n}\nORDER BY ?labelRank ?lang\n\nTry it out↗\nThe same query without aliases\n\nTry it with replacing the property value to wd:P434.\nChange the language codes for labelling. If a certain label does not exist on Wikidata in one of the languages, you will get no label.\n\nImagine you would like to work with the biographical data of photographers connected to Hungary. The following query can show you who has information on Wikidata. You may decide to import this information and use it as a starting point.\n\n# Photographers: citizens of Hungary\n\nSELECT ?item ?itemLabel  ?givenNameLabel ?lastnameLabel ?birthdate ?deathdate ?nationalityLabel ?itemDescription WHERE {\n    ?item wdt:P31 wd:Q5 .                # instance of human\n    ?item wdt:P106/wdt:P279* wd:Q33231.  # occupation,subclass of occupation photographer \n    ?item wdt:P27 wd:Q28.                # country of citizenship is Hungary  \n    optional { ?item wdt:P735 ?lastname . }\n    optional { ?item wdt:P734 ?givenName . }\n    optional { ?item wdt:P569 ?birthdate . }\n    optional { ?item wdt:P570 ?deathdate . }\n    optional { ?item wdt:P27 ?nationality . }\n\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en,hu\" }\n}\n\norder by ?itemLabel\n\nTry it out↗. Beware, that Wikidata is huge, and query may take minutes to run; you often get an error message that your query run out of resources. Then try again.\nOr similarly, with composers connected to Slovakia:\n\n# Composers: citizens of Slovakia\n\nSELECT ?item ?itemLabel  ?givenNameLabel ?lastnameLabel ?birthdate ?deathdate ?nationalityLabel ?itemDescription WHERE {\n    ?item wdt:P31 wd:Q5 .                # instance of human\n    ?item wdt:P106/wdt:P279* wd:Q36834.  # occupation or subclass of occupation that is composer\n    ?item wdt:P27 wd:Q214.               # country of citizenship is Slovakia  \n    optional { ?item wdt:P735 ?lastname . }\n    optional { ?item wdt:P734 ?givenName . }\n    optional { ?item wdt:P569 ?birthdate . }\n    optional { ?item wdt:P570 ?deathdate . }\n    optional { ?item wdt:P27 ?nationality . }\n\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en,sk,de,hu\" }\n}\n\norder by ?itemLabel\n\nTry it out↗",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Wikidata and Other Open Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikibase.html",
    "href": "wikibase.html",
    "title": "4  Wikibase and Enterprise Knowledge Graphs",
    "section": "",
    "text": "4.1 Wikibase\nWikibase is the software that runs Wikidata. Wikidata evolved into a central hub on the web of data and one of the largest existing knowledge graphs, with more than 100 million items maintained by a community effort. Since its launch, an impressive 1.3 billion edits have been made by 20,000+ active users. Today, Wikidata contains information about a wide range of topics such as people, taxons, countries, chemical compounds, astronomical objects, and more. This information is linked to other key data repositories maintained by institutions such as Eurostat, the German National Library, the BBC, and many others, using 6,000+ external identifiers. The knowledge from Wikidata is used by search engines such as Google Search, and smart assistants including Siri,Alexa, and Google Assistant in order to provide more structured results.\nWhile one of the main success factors of Wikidata is its community of editors, the software behind it also plays an important role. It enables the numerous editors to modify a substantial data repository in a scalable, multilingual, collaborative effort.\nWikibase is a software system that help the collaborative management of knowledge in a central repository. It was originally developed for the management of Wikidata, but it is available now for the creation of private, or public-private partnership knowledge graphs. Its primary components are the Wikibase Repository, an extension for storing and managing data, and the Wikibase Client which allows for the retrieval and embedding of structured data from a Wikibase repository. It was developed by Wikimedia Deutschland.\nThe data model for Wikibase links consists of “entities” which include individual “items”, labels or identifier to describe them (potentially in multiple languages), and semantic statements that attribute “properties” to the item. These properties may either be other items within the database, or textual information.\nWikibase has a JavaScript-based user interface, and provides exports of all or subsets of data in many formats. Projects using it include Wikidata, Wikimedia Commons,[5] Europeana’s Eagle Project, Lingua Libre,[6] FactGrid, and the OpenStreetMap wiki.[7]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wikibase and Enterprise Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikibase.html#wikibase",
    "href": "wikibase.html#wikibase",
    "title": "4  Wikibase and Enterprise Knowledge Graphs",
    "section": "",
    "text": "Note\n\n\n\nWikidata itself is a gigantic Wikibase instance. Their user interface is similar, but depending on what the administrator of your Wikibase instance allows you to do, you are likely to have more freedom to edit certain elements, like properties, than on Wikidata. Wikidata must protect the integrity of one of the world’s largest knowledge systems, and does not allow editing access to certain elements.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wikibase and Enterprise Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikibase.html#populating-a-wikibase",
    "href": "wikibase.html#populating-a-wikibase",
    "title": "4  Wikibase and Enterprise Knowledge Graphs",
    "section": "4.2 Populating a Wikibase",
    "text": "4.2 Populating a Wikibase\nWikibase is an open knowledge base or universe when installed. We start populating it with some items. In the Wikidata data model, items are similar to things, and classes are also defined as items.\n\n\n\n\n\n\nNote\n\n\n\nA sandbox instance is a Wikibase instance designated for learning, testing, experimenting. Reprex has created several sandbox instances for onboarding our data curators and for educational purposes. Please see Chapter 5 for getting an account on such an instance.\n\n\n\n4.2.1 Creating entities or items\n\n\n\nSpecial pages ➔ Wikibase ➔ Create a new item\n\n\n\n\n\nIdentical to Wikidata: you must fill out at least the main Label of the item, and a description. We use English (en) as the master language for international cooperations.\n\n\nSuppose you want to make an item or property entity multi-lingual. In that case, you must add at least a new label or description via the Special Pages on the Graphical User Interface (i.e., using your browser.) If you work with our import-export tool or the API, you can set labels and descriptions in several languages in one command.\n\n\n\nYou can reach this form via the Special Pages ➔ Wikibase ➔ Set Item/Property label or et Item/Description link.\n\n\n\n\n4.2.2 Creating properties\nProperties are describing relationships between items. You can create them similary to items, but navigating to Special pages ➔ Wikibase ➔ Create a new property (not item). Properties are far more important than items, because they define the rules of the knowledge base. The type of relationships will allow our artificial intelligence applications to make deductive or inductive new discoveries and expand our knowledge.\nIn our introduction to Wikidata (Section 3.1), you found exactly the same graphical interface to work with items as on Wikibase, but on the public Wikidata instance of Wikibase, you cannot find an add new property button.\n\n\n\n\n\n\nNote\n\n\n\nOn Wikidata, you are not allowed to create new properties: they are created after a consultation with the Wikidata community. The addition of properties determines who the knowledge graph will work in the future.\n\n\nNeedless to say that when you work with a Wikibase instance, you should be also very careful with properties. While changing items usually requires domain-specific knowledge, which you likely possess if you work on an instance, the property sometimes requies knowledge about the information or data model of the instance.\nNot always: some properties are self-explanatory and very easy to create and maintain. For example, the adition of identifiers to other data systems is straightforward. Adding properties that define family relationships (which have their logical rules) requires more careful planning.\n\n\n\nProperties have an extra field that you must fill out: the type of expected data type.\n\n\nProperties have expected data types:\n\nUse a URL for connecting to other ontologies, data models (and add persistent URIs)\nUse item for entities that you want to weave together in the knowledge graph.\nUse literal values like string that for data that will be entered, but not will be placed on a graph.\n\nFor example, if you add Mai Manó as a string, it will be recorded, but you cannot connected with the works of Mai Manó, the photographer. If you create an entity (item) for Mai Manó, you will be able to link this entity to the works of Mai Manó, to his children, to his house.\n\n\n4.2.3 Adding statements\nNow we are ready to start to build an intelligent knowledge base. We connect the person item in our Wikibase via the equivalent class property to the E21_Person definition of the CIDOC CRM. This will allow us to export our knowledge base to a standard museological graph.\n\n\n\n\n\nIn this case, the equivalent class property only accepts URLs. The URI of the CIDOC definition of E_21 Person takes the format of a URL so you can enter it here, but a simple string like E21 would not be allowed.\n\n\n\n\n\n\nNote\n\n\n\nAdding statements is exactly the same procedure on Wikibase as on Wikidata (which is a gigantic Wikibase instance itself.) The only difference is that you can only use properties (or items) that exist on the Wikibase instance or Wikidata. Because Wikibase instances usually should have a different knowledge coverage, some properties and items are not available on others.\n\n\n\n\n4.2.4 Synchronize with Wikidata\nIn our case, we want to be able to pre-fill data from Wikidata, and then, eventually suggest changes in the public Wikidata. This requires adding statements about Wikidata equivalent properties and items when applicable.\n\n\n\nWe created a special property, equivalent Wikidata property, to link the P69 property definition in our Wikibase instance to Wikidata’s equivalent P1709. This will allow us synchronisation among the public Wikidata and our Wikibase.\n\n\n\n\n\nFor items (and classes are defined as items in Wikibase, just like instances of persons), we created a special property equivalent Wikidata item to keep the Person entity (see above its creation) synchronized with Wikidata’s Q5 item.\n\n\nLet us put this all together and create a bibliographic entry. Here we will use a slight deviation from CIDOC, and use the instance of property (equivalently defined in our Wikibase with Wikidata) for class inheritance. When we create a new entity (Manó Mai), we will define this entity as an instance of a person. Persons have birth date, family members, they can create new creative works. In ontologies and in RDF we call these abstract concepts classes.\n We immediately record that our entries about Manó Mai, the great photographer, should be talking about the same person as Wikidata’s Q1163414 document item.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wikibase and Enterprise Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikibase.html#good-practices",
    "href": "wikibase.html#good-practices",
    "title": "4  Wikibase and Enterprise Knowledge Graphs",
    "section": "4.3 Good practices",
    "text": "4.3 Good practices\nLet us consider the creation of an entry for the Slovak composer, \n\n4.3.1 Use of name strings or controlled vocabularies\nIn this case, we would like to code the given name property to Ján. We can do it in two ways: - add the string Ján without further control, or, - add Ján as a controlled string (an item datatype on Wikibase.)\n Unless we can import comprehensive datasets, usually data enrichment is a second step. In such cases, we import first to a name string property given names, locations, venue names, and other important nodes of our knowledge graph.\n The use of controlled vocabularies makes filtering the database easier, and reduces the likelihood of errorneous entries. In the Wikidata data model, we can add a taxonomical class to such controlled vocabulary items. By coding Ján as an instance of the Slovak make given name, we can later search composers or persons easer by this name given name or we can infer that the composer was born as a man.\n\n\n\nThe use of controlled vocabularies (and items) have many advantages.\n\n\nIn this case, we would like to code the given name property to Ján. We can do it in two ways: add Ján as a controlled string (an item datatype on Wikibase) add the string Ján without further control.\nCoding Ján. to Ján must be done with the knowledge of the data curator. We can only make this coding if we know that the string Ján came from a given name (or equivalent) database table column, if indeed it comes from a database of Slovak persons. This is one of the reasons why our bots, i.e., automated importing tools will map given names first to the given name string property.\nSimilar name string properties:\n\nlocation of first performance (string), location of creation (string): The strings Bratislava, Bratislava, CS, Bratislava, SK, Bratislava, Austria-Hungary or map to the item:Bratislava.\nlocation of first performance, location of creation: locations must be items of the class city, town, village (they all have their regional and country entities), or region (they have their country) or country. The city item Bratislava contains the knowledge that this is the current capital city of the Slovak Republic, and it is a former town in Czechoslovakia and Austria-Hungary (it has 232 statements which enrich the concept of Bratislava), and it is connected to lists like List of people from Bratislava.\nvenue of first performance (string): the string Jesuit Church of St. Francis Xavier will need to be matched to a venue item\nvenue of first performance: Jesuit Church of St. Francis Xavier, Skalica, Slovakia as a venue item, which can be a class of building, or an atelier, or a concert hall within a building.\nevent of the first performance (string): Prague Spring International Music Festival—this is not a venue but a festival event.\nevent of the first performance: Prague Spring International Music Festival is a repeating event, and it has its own entity among music festivals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wikibase and Enterprise Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikibase.html#sec-eu-knowledge-graph",
    "href": "wikibase.html#sec-eu-knowledge-graph",
    "title": "4  Wikibase and Enterprise Knowledge Graphs",
    "section": "4.4 The EU Knowledge Graph",
    "text": "4.4 The EU Knowledge Graph\n\n\n\nEU Academy Course: EU Knwoledge Graph\n\n\nBecause of the success of Wikidata, many projects and institutions are looking into Wikibase, the software that runs Wikidata. They aim to reuse the software to construct institutional or cross-institutional, domain-specific knowledge graphs. Several factors make Wikibase attractive:\n\nthe fact that it is a well-maintained open-source software;\nthere is a rich ecosystem of users and tools around it;\nWikimedia Deutschland↗ (WMDE), the maintainer of Wikibase, has made considerable investments in optimising the software’s use outside of Wikidata or other Wikimedia projects;\nThe EU Knowledge Graph↗ runs on Wikibase;\nThe EU Academy and the EU Open Data Portal actively disseminate good practices and know-how on its implementation in cross-institutional data-sharing programs.\n\nOur OpenCollections instances are prepared with a similar mindset to the creation of the EU Knowledge Graph↗. We pre-populate a Wikibase instance from Wikidata about many institutional, geographical or biographical facts of the domain (Diefenbach, Wilde, and Alipio 2021), or with elements of the Wikidata data model and its compatibility classes with other ontologies.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wikibase and Enterprise Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikibase.html#eu-academy-course-on-wikibase",
    "href": "wikibase.html#eu-academy-course-on-wikibase",
    "title": "4  Wikibase and Enterprise Knowledge Graphs",
    "section": "4.5 EU Academy Course on Wikibase",
    "text": "4.5 EU Academy Course on Wikibase\n\n\n\nThe EU Academy course on using Wikibase and Semantic MediaWiki\n\n\nTarget audience\nPolicymakers, public administrators, data maintainers, IT professionals.\nLearning objectives\n\nPros and cons of using Wikibase/SMW for your dataspace\nLessons learnt from projects already using Wikibase/SMW instances\nPractical know-how about setting up a new Wikibase/SMW from scratch\nWhat should be on Wikidata vs in a local Wikibase/SMW\nComparison between Wikibase and SMW\n\nOffered by\nThis content is offered by the European Commission. The European Commission is the European Union’s politically independent executive arm. It is alone responsible for drawing up proposals for new European legislation, and it implements the decisions of the European Parliament and the Council of the European Union.\n\n\n\n\nDiefenbach, Dennis, Max de Wilde, and Samantha Alipio. 2021. “Wikibase as an Infrastructure for Knowledge Graphs: The EU Knowledge Graph.” In ISWC 2021. Online, France. https://hal.science/hal-03353225.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Wikibase and Enterprise Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "sandbox.html",
    "href": "sandbox.html",
    "title": "5  Reprex’s Sandbox",
    "section": "",
    "text": "5.1 Create an Account\nDepending on the type of MediaWiki+Wikibase instance you are using, you may need to create an account to access the site. The process may be less or more strict, depending on how much private data the instance holds.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "sandbox.html#sec-opencollections-create-account",
    "href": "sandbox.html#sec-opencollections-create-account",
    "title": "5  Reprex’s Sandbox",
    "section": "",
    "text": "Access Reprex’s Sandbox Environment. Beware, we have multiple instances, so access the instance with its URL where you have an invitation.\nOn this page, select Request Account.\n\n\n\n\nRequest account - beware, we maintain several sandbox and live production instances, you must navigate to the one wher eyou really want to have this account.\n\n\n\nOn the next page, type in your chosen username and your email address. For a username, use a professional one that is similar to what you use on Keybase, Github, etc. Then confirm by clicking request account again.\n\n\n\n\nUsernames on Wikibase always start wih a capital letter, i.e., Janedoe, or Jane.doe, Or Jane.Doe.\n\n\n\nCheck your email inbox now. You should receive an email with a confirmation link. Click on this confirmation link. (The machine-generated email may easily go to the spam box.)\n\n\n\n\nOften the confirmation mail ends up in your spam.\n\n\n\nAfter you confirm your account request, the administrators of the Wikibase instance will evaluate it. Then, you will receive another email with your login credentials, including your temporary password.\n\n\n\nRevisit the sandbox page and log in. On the login page, type your username and the temporary password you received, then click Log In. You will be automatically taken to the next page, where you must change your password by typing your new permanent password. Provide your new password, then confirm it.\n\n\n\nAll done; you are now logged in to your account.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "sandbox.html#editing-data",
    "href": "sandbox.html#editing-data",
    "title": "5  Reprex’s Sandbox",
    "section": "5.2 Editing data",
    "text": "5.2 Editing data",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "sandbox.html#weaving-data-into-the-knowledge-graph",
    "href": "sandbox.html#weaving-data-into-the-knowledge-graph",
    "title": "5  Reprex’s Sandbox",
    "section": "5.3 Weaving Data Into the Knowledge Graph",
    "text": "5.3 Weaving Data Into the Knowledge Graph\nJust because we edit data in a Wikibase instance, it will not necessarily be more usable than a spreadsheet or a simple local database. If we add 42 without a context, such as age or the number of tracks, these two numeric characters will be only literal numbers. We can increase knowledge by making every point of information a node in the knowledge graph, an edge where new information can flow in.\nIn Wikibase, we call these nodes entities. If we make Albert Einstein an entity instead of the string Albert Einstein, we will be able connect knowledge about his life, his scientific work, proofs, photographs of his lectures, and other forms of knowledge.\nWhen we start importing information into a knowledge graph or begin editing and enriching information within the graph, we are faced with a crucial decision. We must determine which data points, such as cells in an original spreadsheet, or database table, or financial ledger, should be elevated to the status of nodes in the graph. These nodes, or entities, have the potential to develop their own relationships, thereby enriching the overall knowledge graph. Understanding this decision-making process empowers us to effectively utilize Wikibase for our data management needs.\n\n5.3.1 Improving relational databases\nWhen the aim is to improve the data quality, content, or timeliness of a relational database system, the first and most essential candidates to become entities are the database’s primary and secondary keys. To recall our simple example from Section 3.1,\n\n\n\nID\nAuthor\nTitle\n\n\n\n\nMy-01\nMartell, Yann (Q13914)\nLife of Pi (Q374204)\n\n\n…\n…\n…\n\n\nMy-42\nAdams, Douglas (Q42)\nHitchhiker’s Guide to the Galaxy Q25169)\n\n\n…\n…\n…\n\n\n\n\nIf you can connect your My-42 entry with Q25169 on Wikidata, you can import a wealth of information into your private catalogue. And if you add Q42 to the author Douglas Adams, you can import a lot of knowledge, for example, information about his other works or the end of the copyright protection term of these books, after which they will become public domain and free for copying and distribution.\n\nIntuitively, in Wikibase, this means that we “conceptualise” authors and their books. The person known as Douglas Adams becomes a human, a creator, and a writer, with all the properties that writers have… such as books. The Hitchhiker’s Guide to the Galaxy will turn from string into the concept of a Book. As soon as we state that this is a book, not merely a text, we can start adding book-specific knowledge to the Hitchhiker's Guide to the Galaxy book entity: ISBN number, first publication date, translations. And what is most important, we can connect this entity with the author, Douglas Adams, who is no longer just one of the many people who are known by this name, but the person who wrote quiet humorous books.\nConceptualisation is possibly manually, as we have shown in Section 3.1; but usually we do this after data modelling with bulk importing. You tells us what is your data about: books and author, and we import them as Books and Authors, so that we can start to look for more information about these books and authors in various knowledge systems.\n\n\n5.3.2 Improving spreadsheet databases\nSmaller organisations often do not use relational databases; instead, they use Excel or OpenOffice spreadsheets maintained by workers, often for decades. Turning such spreadsheets into knowledge base elements is similar to working with a relational database, but sometimes, it is a smaller and more difficult task.\nWell-organised spreadsheets can be good databases because spreadsheet applications like Excel, OpenOffice, or Google Spreadsheet allow the use of primary and secondary keys by connecting worksheets and the creation of pivot tables.\nThe key challenge with spreadsheets is identifying the Things that should become entities. What is your spreadsheet about? Buildings? Then, addresses and building names should become entities and nodes in the graph. Addresses keep changing, building geometries keep changing, and new additions are built or demolished. Street names change. Even city names change; cities merge and divide.\n\n\n5.3.3 Improving annotated text, legal documents, lab notes, regulatory filings\n\n\n5.3.4 Creating new indicators",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "opencollections.html",
    "href": "opencollections.html",
    "title": "6  OpenCollections",
    "section": "",
    "text": "6.1 Going Beyond Wikibase\nOur system is inspired by the WB-CIDOC model developed at the University of Helsinki for translating knowledge stored in Wikibase into the statements described with the CIDOC ontology used by intelligent cultural heritage systems (Kesäniemi, Koho, and Hyvönen 2022). CIDOC is a modern, events-based ontology that allows building trustworthy inference and deduction AI engines.\nThe WB-CIDOC provides rules for writing data into Wikibase in a way that translates correctly into an event-based model, but we find its use counter-intuitive and laborious for domain expert data curators.\nMost domain experts would think that a biographical entity of Albert Einstein should have a birthday property with the date of March 14, 1879, while an event-based ontology would create first an abstract event, the Birth of Albert Einstein, with a timespan of March 14, 1879, 0:00 to 23.59. It is far easier to search for parallel events in this time window or connect further information— like persons present at birth, certificates created, etc.—than to connect this information to a simple, literal date.\nDomain-level experts like copyright specialists, ESG experts, musicologists, bank professionals, and other users usually need formal computer- or information science training and find the entity-based approach closer to real-world experience. We design our knowledge-base instances with hooks for more complex knowledge-base ontologies. This allows our users to review the information in a natural, entity-based format; our intelligent applications translate the information to more complex structures, such as event-based conceptual models, to allow more reasoning capacity for our AI systems.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OpenCollections</span>"
    ]
  },
  {
    "objectID": "opencollections.html#going-beyond-wikibase",
    "href": "opencollections.html#going-beyond-wikibase",
    "title": "6  OpenCollections",
    "section": "",
    "text": "6.1.1 Translation to more complex data models\nOur system is inspired by the WB-CIDOC model developed at the University of Helsinki for translating knowledge stored in Wikibase into the statements described with the CIDOC ontology used by intelligent cultural heritage systems (Kesäniemi, Koho, and Hyvönen 2022). CIDOC is a modern, events-based ontology that allows building trustworthy inference and deduction AI engines.\nThe WB-CIDOC provides rules for writing data into Wikibase in a way that translates correctly into an event-based model, but we find its use counter-intuitive and laborious for domain expert data curators.\nMost domain experts would think that a biographical entity of Albert Einstein should have a birthday property with the date of March 14, 1879, while an event-based ontology would create first an abstract event, the Birth of Albert Einstein, with a timespan of March 14, 1879, 0:00 to 23.59. It is far easier to search for parallel events in this time window or connect further information— like persons present at birth, certificates created, etc.—than to connect this information to a simple, literal date.\n\n\n\n\n\n\nNote\n\n\n\nDomain-level experts like copyright specialists, ESG experts, musicologists, bank professionals, and other users usually need formal computer- or information science training and find the entity-based approach closer to real-world experience. We design our knowledge-base instances with hooks for more complex knowledge-base ontologies. This allows our users to review the information in a natural, entity-based format; our intelligent applications translate the information to more complex structures, such as event-based conceptual models, to allow more reasoning capacity for our AI systems.\n\n\n\n\n6.1.2 Record-keeping and retention\nNational archives play a crucial role in preserving the collective memory and history of a nation. Connecting national archives to institutional enterprise record-keeping systems has many advantages.\n\nContextualising institutional or enterprise records: Private organisations and users cannot copy all legally or historically relevant documents in their inventory. Connecting to memory institutions, such as records or legal databases, allows one to find precedents and understand one and one’s own historical records in context without the need to hoard information on an excessive scale. Just the way we do not need to burden our office bookshelves with bilingual dictionaries or printed copies of changing regulations, we can further lower the burden by making our records system compatible with national records.\nRecord retention and public archiving is a regulated process that serves as the foundation of many business processes’ regulatory or assurance oversight. Businesses often must deposit copies of legally important disclosures and certificates at public bodies. Larger institutions, primarily if they work for the public benefit, usually have a legal mandate to place some of their documents into a public archive. Private persons and companies often donate documents to such archives when they want to be credited with their work, intellectual property, or the value of their activities.\n\nBecause OpenCollections is based around a document-based database, it is very well suited to support document exchanges between private institutions (e.g., the exchange of technical and delivery documentation along the supply chain), public institutions (e.g., the exchange of public documents), and public-private exchanges.\nWe provide mappings, software tools and training to apply Records in Context (RiC), a novel ontology released in 2023 after over a decade’s work to replace the four international standards on archiving. The last international standards on the topic were created before the commercial Internet; RiC provides backwards compatibility to millennia of historical records, corporate document inventories, and physical data vaults on one hand, and opens up the use of modern knowledge graphs to link information in the archives with your documents in use. RiC is the gateway to corporate and institutional textual big data.\n\n\n6.1.3 Data catalogues, and the meaning of data tables\nFollowing the DCAT-AP specification of the EU Open Data Portal and Stat-DCAT-AP to offer full compatibility with European statistical portals and open data portals, we translate information about datasets, data codes and structures, and variable descriptions. This translation works with few limitations for global resources beyond Europe. It connects corporate or institutional datasets and accounts with statistical and national accounts data from public sources, offering unparalleled ease in creating economic or sustainability-controlling applications.\n\n\n6.1.4 Collections and inventories\n\n\n\n\nKesäniemi, Joonas, Mikko Koho, and Eero Hyvönen. 2022. “Using Wikibase for Managing Cultural Heritage Linked Open Data Based on CIDOC CRM.” In New Trends in Database and Information Systems, edited by Silvia Chiusano, Tania Cerquitelli, Robert Wrembel, Kjetil Nørvåg, Barbara Catania, Genoveva Vargas-Solar, and Ester Zumpano, 542–49. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-15743-1_49.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OpenCollections</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Data Documentation Initiative. 2020. “DDI Lifecycle\n(3.3) Documentation.” https://ddi-lifecycle-documentation.readthedocs.io/en/latest/index.html.\n\n\nDiefenbach, Dennis, Max de Wilde, and Samantha Alipio. 2021.\n“Wikibase as an Infrastructure for Knowledge Graphs: The\nEU Knowledge Graph.” In ISWC\n2021. Online, France. https://hal.science/hal-03353225.\n\n\nKesäniemi, Joonas, Mikko Koho, and Eero Hyvönen. 2022. “Using\nWikibase for Managing Cultural Heritage Linked Open Data Based on\nCIDOC CRM.” In New Trends in\nDatabase and Information Systems, edited by Silvia Chiusano, Tania\nCerquitelli, Robert Wrembel, Kjetil Nørvåg, Barbara Catania, Genoveva\nVargas-Solar, and Ester Zumpano, 542–49. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-031-15743-1_49.\n\n\nUNECE. 2014. “Generic Statistical Information Model.\nGSIM V2.0 Documents. UNECE Statswiki.”\n2014. https://statswiki.unece.org/display/gsim/GSIM+v2.0+documents.\n\n\nVardigan, Mary, Pascal Heus, and Wendy Thomas. 2008. “Data\nDocumentation Initiative: Toward a Standard for the Social\nSciences.” International Journal of Digital Curation 3\n(1): 107–13.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "questionbank.html",
    "href": "questionbank.html",
    "title": "Appendix A — Question Bank Items In Wikibase",
    "section": "",
    "text": "A.1 Need for Questions\nThe need for questions arises from the fact that you want to collect some data systematically, either in an online or face-to-face questionnaire, in a structured interview, or in making data requests to an API or a form to record repeated answers to this question. After statistical manipulations, such as summarising and averaging, the responses will create empirical variables in a dataset. You can rely on existing data and expand your knowledge utilising already collected (open) data if you use the same questions that other researchers or statisticians have used before you.\nWithout addressing the theory of data harmonisation, these are the steps you are likely to make:\nOur question bank is designed to be searchable by concepts, question types, question labels or question texts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question Bank Items In Wikibase</span>"
    ]
  },
  {
    "objectID": "questionbank.html#question-types",
    "href": "questionbank.html#question-types",
    "title": "Appendix A — Question Bank Items In Wikibase",
    "section": "A.2 Question Types",
    "text": "A.2 Question Types\nWhat is a survey question after all? DDI organises questions into 3+1 hierarchical levels.\n\nQuestion: A question and its answer options formulated in at least one natural language, for example, Please tell me to what extent you agree or disagree with the following statement: “I trust that products carrying the EU ecolabel are environmentally-friendly.” [ ] Totally agree [ ], Tend to agree [ ], Tend to disagree [ ], Totally disagree [ ], DK. In this example DK refers to declined to comment on the question, or a refusal to answer this question.\nQuestionItem: the concept (i.e., unemployment, or plastic bags) being measured by the question, text for the question, response domain information, clarifying instructions, external aids (clarifying objects used in presenting the question to the respondent), Input and Output Parameters and Bindings, allowed response cardinality and estimation of the time required to respond.\nQuestionBlock: This structure is intended to bundle together a set of questions (items and/or grids) that have meaning only about a specified object expressed as the evaluation material. This form of question set is common in educational testing where a text, image, or other material is provided, and the respondent is asked questions specific to the material. For example, a portion of a play script is provided, and the respondent is asked questions concerning the dialogue and/or stage directions provided in the script. Note that the intent of QuestionBlock is not to bundle together a set of questions that are commonly used together or used in a specified order.\nQuestionGroup is only for administrative purposes.\n\n\nA.2.1 Model question\nOne other way to make questions and resulting responses and their statistically processed variables comparable is to ask questions about different concepts in a same way? A standard quesiton in a Cultural Access and Participation Survey is:\nHow many times in the past 12 months have you been to ... ... a concert? ... cinema? ... church?\nWhile people may have recollection biases about the 12 months, and may use a bit differently the concept of concert or cinema, because of the same syntax, context, we can assume that their responses are comparable. In this case, How many times in the past 12 months have you been to ... is a model question.\nA model question is a question template that can create simple questions or question items in question grids or blocks.\n\nURI: Q127\nlabel: Trust in EU ECOLABEL [model]\nquestionText (description): Please tell me to what extent you agree or disagree with the following statement: “I trust that products carrying the EU ecolabel are environmentally-friendly.” [ ] scale\n\nAnd a question based on a model question, taken from\n\nQID: https://reprexbase.eu/demowiki/index.php?title=Item:Q111\nlabel: Trust in EU ECOLABEL\nquestionText (description): Please tell me to what extent you agree or disagree with the following statement: “I trust that products carrying the EU ecolabel are environmentally-friendly.” [ ] Totally agree [ ], Tend to agree [ ], Tend to disagree [ ], Totally disagree [ ], DK\nvariable representation: scale representation base type\nstudy (DDI): Eurobarometer 88.1 (2017)\n\nOur model questions follow one of the following formats:\n\nquestionText (description), [ ] concept, where the question connects to a concept, such as environmental protection (Q131).\nquestionText (description), [ ] scale, where the answers are on a scale, for example Estimated number of employees in FTE [model] (Q123)\nquestionText (description), [ ] category, where the answer options are categories, for example: Reduced use of single use plastic bags [model] (Q112)\nquestionText (description), [ ] ranking, where the respondent has to create a rank from the answer options, for example: Important environmental issue [model] (Q143)\nquestionText (description), [ ] concept, [ ] scale, where beside the model question there are other sub-questions as well, for example: Important for reduction of plastic [model] (Q128)\n\nBased on the DDI-Lifecycle model we could generate differently structured model questions, and if there will be user need, we will introduce further question templates.\nThe DDI-Discovery ontology requires the questions to take this format:\n\nPlease tell me to what extent you agree or disagree with the following statement: “I trust that products carrying the EU ecolabel are environmentally-friendly.” [ ] Totally agree [ ], Tend to agree [ ], Tend to disagree [ ], Totally disagree [ ], DK.\n\nThis is a good representation to for an existing questionnaire, but it is not really good for a questionbank, because in some cases, the agreement scale may be a 3-level, in others, a 5-level agreement scale:\n\nPlease tell me to what extent you agree or disagree with the following statement: “I trust that products carrying the EU ecolabel are environmentally-friendly.” [ ], Agree [ ], [ ], Disagree [ ], DK\n\nWe can argue that the responses are still comparable, but [ ] Totally agree [ ], Tend to agree [ ] should be added together for a broader [ ] Agree category if one survey uses the 5-scale version of the response scale while the other uses the 3-scale (agree, disagree, decline) version.\nThis is why we separately record the model question, the subquestions and the answer options.\n\n\nA.2.2 Simple, Multiple Choice and Matrix Questions\nDifferent question types have different elements. Some questions consist of one question, others have group questions and several sub-questions.\nA question might consists of the following elements:\n[Model question] +  [Question Items] +  [Answer options]\n\nAll elements should be added separately to the Wikibase\nThe format changes based on the type of the question:\n\nsimple question: [Model question] + [Answers]\nmultiple choice question: [Model question] + [Question Items]\nmatrix question: [Model question] +  [Question Items] +  [Answer options]\n\n\nLet’s see how to load into Wikibase:\n\nSimple Questions\n\n\n\nMatrix Questions\nMultiple Choice Questions\n\n\nA.2.2.1 Simple Questions\nIn case of Simple Questions there’s only one question.\n\n\n\n\n\n\nNote\n\n\n\nFor a clearer definition, see the disco:Question class.\n\n\n The format of a simple question is: [Model question] + [Answer options]\nLet’s see how to create a simple question entry in Wikibase. Go to “Special Pages”\n\n\n\n\n\nScroll down and select: Create a New Item\n\n\n\n\n\nFill the form with the question’s data:\n\nLanguage ▷ Choose the language (en)\nLabel - Give a short name for the question\nDescription - Enter the question itself in the format specified above.\nAliases - leave it empty\n\n\n\n\n\n\nClick Create.\nThe question now is created on Wikibase.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The system assigns a unique ID to every entry. In our example the ID is Q111.\n\n\n\n\nA.2.2.2 Matrix Questions\nMatrix questions have:\n\na model question\nseveral question items\nanswer options\n\n\n\n\nThis question is taken from the question block D (QD) from the Eurobarometer 88.1 study.\n\n\nFollowing the functions “Special Pages” ▷ “Create New Item” you should feed into Wibikbase separately the model question and the questions items.\n\n\n\n\n\nQ128 is the model question, which follows the structure:\n\nLanguage ▷ Choose the language (en)\nLabel - questionName + [model] - Important for reduction of plastic [model]\nquestionText (description)\n\nIn your opinion, how important is each of the following in reducing plastic waste and littering?\n[ ] concept - stands for the question items\n[ ] scale - stands for the answer options, which follow a scale\n\nAliases - leave it empty\n\n\n\n\n\n\nQ140 is a question item, which follows the structure:\n\nLanguage ▷ Choose the language (en)\nlabel: Important for reduction of plastic - collection facilities\nquestionText (description)\n\nmodel question - In your opinion, how important is each of the following in reducing plastic waste and littering?\nquestion item: [ ] Local authorities should provide more and better collection facilities for plastic waste\n[ ] scale - stands for the answer options, which follow a scale\n\nAliases - leave it empty\n\n\n\n\n\n\n\nWhen creating a “question item”, using statements, always connect the “question item” to the “model question”\n\n\n\n\n\n\n\n\n\n\n\n\n\nA.2.2.3 Multiple Choice Questions\nMultiple Choice questions have:\n\na model question\nseveral question items\n\n\n\n\nThis question is taken from the question block D (QD) from the Eurobarometer 88.1 study.\n\n\nFollowing the functions “Special Pages” ▷ “Create New Item” you should feed into Wibikbase separately the model question and the questions items.\n\n\n\n\n\nQ143 is the model question, which follows the structure:\n\nLanguage - Choose the language (en)\nLabel - questionName + [model] - Important environmental issue``[model] \nquestionText (description)\n\nFrom the following list, please pick the four environmental issues which you consider the most important.\n[ ] ranking - stands for the question items\n\nAliases: leave it empty\n\n\n\n\n\n\nQ144 is a question item, which follows the structure:\n\nLanguage ▷ Choose the language (en)\nlabel: Important for reduction of plastic - collection facilities\nquestionText (description)\n\nmodel question - From the following list, please pick the four environmental issues which you consider the most important.\nquestion item - [ ] Decline or extinction of species and habitats, and of natural ecosystems (forests, fertile soils)\n\nAliases - leave it empty\n\n\n\n\n\n\n\nWhen creating a “question item”, using statements, always connect the “question item” to the “model question”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question Bank Items In Wikibase</span>"
    ]
  },
  {
    "objectID": "questionbank.html#add-metadata-statements-to-your-questions",
    "href": "questionbank.html#add-metadata-statements-to-your-questions",
    "title": "Appendix A — Question Bank Items In Wikibase",
    "section": "A.3 Add Metadata Statements to your Questions",
    "text": "A.3 Add Metadata Statements to your Questions\nUsing Wikibase’s “statements” feature you can link different type of information to your questions.\nYou need to add further metadata statements to the question bank item. Metadata is a statement about the data. We are adding standard, basic statements in subject, predicate, and object (triplet) format to each question bank item.\nIN the following this guide explain how to add information about:\n\nquestionnaire classes\nvariable representation (P265): a DDI-Lifecycle category for the creation of variables from the answer options, for example\nstudy (DDI) P270: the study where you can find this (model) question (item). In DDI, a study represents the process by which a data set was generated or collected (in a survey). For example, Eurobarometer 88.1 (2017) Q139\nrelated survey concept (P267): a concept that a study (group), question (group) or question item aims to measure, for example environmental protection (Q131).\n\n\nA.3.1 Questionnaire Classes\nLet’s start by specifying the entry we created as model question or question item.\nSpecify the entries created as model question or question item.\n\nSelect +add statement.\nUsing the instance of property, which is defining the taxonomical class of the entered item (in this case, a question.)\nIn case of model questions, define them as model question.\nIn case of question item, define them as question items.\n\n\n\n\n\n\n\n\n\n\n\n\nIn the case of questions items, always link them to the appropriate model questions using the model question (P266) property.\n\n\n\n\n\n\n\n\nA.3.2 Variable Representation\nWhen the questionnaire will be filled out in a raw dataset, each response of a question(item) will be translated into a variable. We need to define how we want to represent those answers in the resulting output dataset. (See DDI 3.3 (2020) documentation - Variable Value Representation and Question Response Domain)\nUsing statements you can define the representation of the variables. You can choose from the following categories:\n\ncategory representation base type: if the answers are categories (for example: [ ] Female, [ ] Male, [ ] Prefer not to say)\ncategory representation base type with a scale: if the answers are categories and follow a scale (for example: [] Very important, [ ] Fairly important, [ ] Not very important, [ ] Not at all important. )\nranking representation base type: the respondant must rank the answer options, like 1st, 2nd, 3rd, etc.\nnumeric variable representation base type: the answer should be a number, for example, the age of the respondant as an integer number or a postal code in a country where postal codes contain only numeric digits, f.e., 1051.\ntextual variable representation base type: the answer should be some text, for example, and open answer, or a geographical location typed as a simple text, for example, Bratislava.\n\n\n\n\n\n\n\n\nA.3.3 Define the source study\n\n\n\n\n\n\nTip\n\n\n\nFor further details, please check the disco:Study class.\n\n\nWith the study (DDI) P270 property you must link as a statement the study where you found the (model) question (item).\n\n\n\n\n\nAn example for a study: Eurobarometer 88.1 (2017) Q139\n\n\n\n\n\n\nNote\n\n\n\nNote: If the study is not yet in Wikibase, you can create an entry for it using the Create a New Item function.\n\n\n\n\nA.3.4 Add related concept\nWith the related survey concept (P267) property you can link concepts, that a study (group), question (group) or question item aims to measure, for example environmental protection (Q131).\n\n\n\n\n\nWhere are the related concepts coming from?\n1. The best case is that you use a widely accepted conceptualisation (ontology item) of your domain. For example, we took the study (Q149) concept from the DDI-Discovery (disco) ontology. You can connect statements of equivalence to a well-defined ontology via equivalent class (P69). In other words, our Q149 entity is equivalent to DDI’s Study.\n2. If there are no accepted ontologies or you are uncertain, it is a very good practice to use a concept definition from Wikidata. Even in the case of an ontological definition, adding the Wikidata QID is a great idea because Wikidata connects equivalent definitions across various domains’ ontologies. You can make a statement about an equivalent Wikidata URI (for an item) by Wikibase URI (P73). See, for example plastic (Q148), Wikibase URI (P73), https://www.wikidata.org/wiki/Q11474, meaning that our plastic definition is equivalent to the Wikidata definition of plastic.\n3. You can create your definition if you are still looking for a suitable definition in an accepted ontology or on Wikidata. For this, you should create a definition in Wikibase (as a new item.) See, for example, model question (Q126), which is our own proprietary definition until we find a more consensual one.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question Bank Items In Wikibase</span>"
    ]
  },
  {
    "objectID": "questionbank.html#add-the-questiontext-translations",
    "href": "questionbank.html#add-the-questiontext-translations",
    "title": "Appendix A — Question Bank Items In Wikibase",
    "section": "A.4 Add the questionText translations",
    "text": "A.4 Add the questionText translations\nOn Wikibase you can add different language versions to the same question.\nTo do so, go to “Special Pages”\n\n\n\n\n\nScroll down and select: “Set Item/Property Description”\n\n\n\n\n\nFill the form:\n\nID - The QiD of the question\nLanguage code - the new language you want to input the question\nDescription - The question itself in the new language\n\n Select “Set Description”.\nThe entry is now updated with another language.\n\n\n\n\nData Documentation Initiative. 2020. “DDI Lifecycle (3.3) Documentation.” https://ddi-lifecycle-documentation.readthedocs.io/en/latest/index.html.\n\n\nHartmann, Thomas, Sarven Capadisli, Franck Cotton, Richard Cyganiak, Arofan Gregory, Benedikt Kämpgen, Olof Olsson, Heiko Paulheim, Joachim Wackerow, and Benjamin Zapilko. 2024. “DDI-RDF Discovery Vocabulary. A Vocabulary for Publishing Metadata about Data Sets (Research and Survey Data) into the Web of Linked Data.” Edited by Thomas Hartmann, Richard Cyganiak, Joachim Wackerow, and Benjamin Zapilko. W3C. https://rdf-vocabulary.ddialliance.org/discovery.html.\n\n\nUNECE. 2014. “Generic Statistical Information Model. GSIM V2.0 Documents. UNECE Statswiki.” 2014. https://statswiki.unece.org/display/gsim/GSIM+v2.0+documents.\n\n\nVardigan, Mary, Pascal Heus, and Wendy Thomas. 2008. “Data Documentation Initiative: Toward a Standard for the Social Sciences.” International Journal of Digital Curation 3 (1): 107–13.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question Bank Items In Wikibase</span>"
    ]
  },
  {
    "objectID": "musicapi.html",
    "href": "musicapi.html",
    "title": "Appendix B — Variables in Music Databases",
    "section": "",
    "text": "B.1 String versus item\nWhenever possible, we want to refer to well-defined nodes in the knowledge graph. For example, our entry Slovakia (Q79) states that it is equivalent with Slovakia (Q214) on Wikidata, and Wikidata connects plenty of metadata to this concept: the geographical boundaries, the fact that it is an independent state since 1993, it predecessors, capital, etc.\nOur aim is to have a rich and standardised description to each variable, and as much as possible, to very constant (or attribute.) Katarína Kubošiová is a Slovak singer-songwriter, also known as Katarzia. To avoid any ambigouity with other people potentially called Katarína Kubošiová or Katarzia, we would like to refer to her with a globally unique identifier. Her ISNI identifier (ISNI: ) is isni: 0000000467220673, which identifies her with global clarity.\nThe metadata enrichment is possible to make data points into nodes. For example, if we conceptualise Slovakia into a node, than we can connect to this node sound recordings (regardless if they have a Slovak or English-language title) if they were registered with the Slovak national ISRC registrant’s SK prefix. We can connect Katarína Kubošiová, Katarzia, SK, Slovakia in a graph to the concept of Slovakia with less or more clarity; in this case, for example, defining that a sound recording was registered in Slovakia, or the artist known as Katarzia was born in Slovakia or sung in the Slovak language.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Variables in Music Databases</span>"
    ]
  },
  {
    "objectID": "musicapi.html#string-versus-item",
    "href": "musicapi.html#string-versus-item",
    "title": "Appendix B — Variables in Music Databases",
    "section": "",
    "text": "Slovakia (Q79) is a well-defined node in our Wikibase graph.\nSlovakia as a string is not well-defined; it can only be understood if we add \"Slovakia\"@en a reference to the natural language of the string.\n\n\n\n\n\nB.1.1 1. Access Wikibase\nLogin in with you account to Wikibase.\n\n\nB.1.2 2. Create a New Item\nGo to Special Pages\n\n\n\n\n\nScroll down and select: Create a New Item\n\n\n\n\n\nFill the form with the item’s data:\n\nLanguage - Choose the language (en)\nLabel - Give a short name for the node, for example, Katarína Kubošiová\nDescription - Enter the item description, for example Singer-songwriter born in the Slovak Republic\nAliases - you can add Katarzia or any other known names here.\n\n\n\n\n\n\nClick Create.\nThe item now is created on Wikibase. For each concept that you want to use in your research, its documentation should be present. For key persons, names, musical works, it is also advisable to have an item defined.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The system assigns a unique ID to every entry. For example, in our system, the ID of Ján Levoslav Bella (Slovak conductor, composer and educator), aslo known under the alias with no Slovak special characters as Jan Levoslav Bella is Q93. With Q93 you cannot make the mistake of confusing the fact that Ján Levoslav Bella is the same person as Jan Levoslav Bella.\n\n\n\n\nB.1.3 3. Add Metadata Statements\nYou need to add further metadata statements to the question bank item. Metadata is a statement about the data. We are adding standard, basic statements in subject, predicate, and object (triplet) format to each question bank item.\n\nB.1.3.1 Variable Representation\nDDI has standard variable representation definitions. When a questionnaire will be filled out in a raw dataset, or data will be systematically queried from and API, each response will be translated into a variable. We need to define how we want to represent those answers in the resulting output dataset. (See DDI 3.3 (2020) documentation - Variable Value Representation and Question Response Domain)\nUsing statements you can define the representation of the variables. You can choose from the following categories:\n\ncategory representation base type: if the answers are categories (for example: [ ] Female, [ ] Male, [ ] Prefer not to say)\ncategory representation base type with a scale: if the answers are categories and follow a scale (for example: [] Very important, [ ] Fairly important, [ ] Not very important, [ ] Not at all important. )\nranking representation base type: the respondant must rank the answer options, like 1st, 2nd, 3rd, etc.\nnumeric variable representation base type: the answer should be a number, for example, the age of the respondant as an integer number or a postal code in a country where postal codes contain only numeric digits, f.e., 1051.\ntextual variable representation base type: the answer should be some text, for example, and open answer, or a geographical location typed as a simple text, for example, Bratislava.\n\n\n\n\n\n\n\n\nB.1.3.2 Define the source study\n\n\n\n\n\n\nTip\n\n\n\nFor further details, please check the disco:Study class.\n\n\nWith the study (DDI) P270 property you must link as a statement the study where you found the concept definition. If it was a formal ontology, or Wikibase, use different properties (see below).\n\n\n\n\n\nAn example for a study: Eurobarometer 88.1 (2017) Q139\n\n\n\n\n\n\nNote\n\n\n\nNote: If the study is not yet in Wikibase, you can create an entry for it using the Create a New Item function.\n\n\n\n\nB.1.3.3 Add related concept\nWhere are the related concepts coming from?\n\nThe best case is that you use a widely accepted conceptualisation (ontology item) of your domain. For example, we took the duration (Q132) concept from the Music Ontology. You can connect statements of equivalence to a well-defined ontology via equivalent class (P69). In other words, our Q149 entity is equivalent to Music Ontology’s duration (in short: mo:duration.)\nIf there are no accepted ontologies or you are uncertain, it is a very good practice to use a concept definition from Wikidata. Even in the case of an ontological definition, adding the Wikidata QID is a great idea because Wikidata connects equivalent definitions across various domains’ ontologies. You can make a statement about an equivalent Wikidata URI (for an item) by Wikibase URI (P73). See, for example duration (Q132), Wikibase URI (P73), https://www.wikidata.org/wiki/Q16038819, meaning that our plastic definition is equivalent to the Wikidata definition of plastic (which has a QID of Q16038819 on Wikidata, and it received the QID of Q132 on our Wikibase instance.)\nYou can create your definition if you are still looking for a suitable definition in an accepted ontology or on Wikidata. For this, you should create a definition in Wikibase (as a new item.) See, for example, model question (Q126), which is our own proprietary definition until we find a more consensual one.\n\n\n\n\nB.1.4 Add national langugage translations to your concept\nOn Wikibase you can add different language versions to the same question.\nTo do so, go to Special Pages\n\n\n\n\n\nScroll down and select: Set Item/Property Description\n\n\n\n\n\nFill the form:\n\nID - The QiD of the question (for example, if you want to add a Dutch description to Ján Levoslav Bella, i.e., Slovak conductor, composer and educator, you must reference Q93.\nLanguage code - the new language you want to input the question, in this case, nl.\nDescription - Write a short definition (up to 250 characters) in the new language.\n\n Select “Set Description”.\nThe entry is now updated with another language label or description.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Variables in Music Databases</span>"
    ]
  },
  {
    "objectID": "musicapi.html#example",
    "href": "musicapi.html#example",
    "title": "Appendix B — Variables in Music Databases",
    "section": "B.2 Example",
    "text": "B.2 Example\n\nB.2.1 1. Access Wikibase\nLogin in with you account to Wikibase.\n\n\nB.2.2 2. Create a New Question Item\n\nFor a clearer definition, see the disco:Question class.\n\nGo to “Special Pages”\n\n\n\n\n\nScroll down and select: “Create a New Item”\n\n\n\n\n\nFill the form with the question’s data:\n\nLanguage - Choose the language (en)\nLabel - Give a short name for the question\nDescription - Enter the question itself in the format specified above.\nAliases - leave it empty\n\n\n\n\n\n\nClick Create.\nThe question now is created on Wikibase.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The system assigns a unique ID to every entry. In our example the ID is Q111.\n\n\n\n\nB.2.3 3. Add Metadata Statements\nYou need to add further metadata statements to the question bank item. Metadata is a statement about the data. We are adding standard, basic statements in subject, predicate, and object (triplet) format to each question bank item.\n\nB.2.3.1 Questionnaire Classes\nLet’s start by specifying the entry we created as a question, model question or question item.\nSelect +add statement.\n\n\n\n\n\nUsing the instance of property, which is defining the taxonomical class of the entered item (in this case, a question.)\n\n\n\n\n\n\n\nB.2.3.2 Variable Representation\nDDI has standard variable representation definitions. When a questionnaire will be filled out in a raw dataset, or data will be systematically queried from and API, each response will be translated into a variable. We need to define how we want to represent those answers in the resulting output dataset. (See DDI 3.3 (2020) documentation - Variable Value Representation and Question Response Domain)\nUsing statements you can define the representation of the variables. You can choose from the following categories:\n\ncategory representation base type: if the answers are categories (for example: [ ] Female, [ ] Male, [ ] Prefer not to say)\ncategory representation base type with a scale: if the answers are categories and follow a scale (for example: [] Very important, [ ] Fairly important, [ ] Not very important, [ ] Not at all important. )\nranking representation base type: the respondant must rank the answer options, like 1st, 2nd, 3rd, etc.\nnumeric variable representation base type: the answer should be a number, for example, the age of the respondant as an integer number or a postal code in a country where postal codes contain only numeric digits, f.e., 1051.\ntextual variable representation base type: the answer should be some text, for example, and open answer, or a geographical location typed as a simple text, for example, Bratislava.\n\n\n\n\n\n\n\n\nB.2.3.3 Define the source study\n\n\n\n\n\n\nTip\n\n\n\nFor further details, please check the disco:Study class.\n\n\nWith the study (DDI) P270 property you must link as a statement the study where you found the (model) question (item).\n\n\n\n\n\nAn example for a study: Eurobarometer 88.1 (2017) Q139\n\n\n\n\n\n\nNote\n\n\n\nNote: If the study is not yet in Wikibase, you can create an entry for it using the Create a New Item function.\n\n\n\n\nB.2.3.4 Add related concept\nWhere are the related concepts coming from?\n\nThe best case is that you use a widely accepted conceptualisation (ontology item) of your domain. For example, we took the duration (Q132) concept from the Music Ontology. You can connect statements of equivalence to a well-defined ontology via equivalent class (P69). In other words, our Q149 entity is equivalent to Music Ontology’s duration (in short: mo:duration.)\nIf there are no accepted ontologies or you are uncertain, it is a very good practice to use a concept definition from Wikidata. Even in the case of an ontological definition, adding the Wikidata QID is a great idea because Wikidata connects equivalent definitions across various domains’ ontologies. You can make a statement about an equivalent Wikidata URI (for an item) by Wikibase URI (P73). See, for example duration (Q132), Wikibase URI (P73), https://www.wikidata.org/wiki/Q16038819, meaning that our plastic definition is equivalent to the Wikidata definition of plastic (which has a QID of Q16038819 on Wikidata, and it received the QID of Q132 on our Wikibase instance.)\nYou can create your definition if you are still looking for a suitable definition in an accepted ontology or on Wikidata. For this, you should create a definition in Wikibase (as a new item.) See, for example, model question (Q126), which is our own proprietary definition until we find a more consensual one.\n\n\n\n\nB.2.4 Add the questionText translations\nOn Wikibase you can add different language versions to the same question.\nTo do so, go to “Special Pages”\n\n\n\n\n\nScroll down and select: “Set Item/Property Description”\n\n\n\n\n\nFill the form:\n\nID - The QiD of the question\nLanguage code - the new language you want to input the question\nDescription - The question itself in the new language\n\n Select “Set Description”.\nThe entry is now updated with another language.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Variables in Music Databases</span>"
    ]
  },
  {
    "objectID": "questionbank.html#need-for-questions",
    "href": "questionbank.html#need-for-questions",
    "title": "Appendix A — Question Bank Items In Wikibase",
    "section": "",
    "text": "For example, if you need data on reusable plastic bags, you need to find a widely shared definition of plastic, reusability and bags.\nYou should consult a database or a question bank to find out if others have already asked about reusable plastic bags.\nIf you formulate a questionnaire using the exact wording and answering options as earlier surveys on attitudes to reusable plastic bags, you will be able to compare the results. So, you need access to question forms (question texts) and answer options.\nIf you work on an international project or want a global comparison, you will need to ensure that the question texts and answer options are translated very similarly and understood equally in different languages.\nAs a practical last step, the responses must be coded the same way as in international data repositories; for example, female respondents are coded with F in most statistical data repositories, even if the word ‘female’ may start with a different letter in many languages.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question Bank Items In Wikibase</span>"
    ]
  }
]