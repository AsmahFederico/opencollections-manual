[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenCollections Manual",
    "section": "",
    "text": "Introduction\nReprex’s new OpenCollections system wants to help small and large enterprises work with big data without huge investments into data infrastructure. OpenCollections is a collaborative tool that enables owners of small, local databases to remain competitive in training or AI in the age of big data. It helps to fill your databases with up-to-date information, find and correct errors in your databases, and connect your database entries to new information as you need them without further IT and data investments.\nOpenCollections aims to link various open, public, and private databases and let the knowledge flow among them. OpenCollections is not a database system but a knowledge system that helps to fill up databases with the latest information, find errors in private databases, or support the human-in-the-loop process of overseeing what AI systems are doing on your behalf or perhaps against you.\nOpenCollections is built around Wikibase, the open-source knowledge management system that was originally developed to maintain the world’s most extensive open knowledge graph, Wikidata. Wikidata evolved into a central hub on the web otf data and it is one of the largest existing knowlege graphs, and perhaps the best known open one. It is synchronised with knowledge from respected public institutions like Eurostat, the German National Library or BBC, and it is one of the backbones of many web services like Google Search. Wikibase is scalable to very big graphs, and it has many thousand users with a simple and intuitive user interface.\nWe are augmenting Wikibase with further software components developed by Reprex that allow its smooth operation in various institutional and enterprise settings. The choice of Wikibase as a core components allows an easy onboarding; and knowledge coordination teams can easily find new colleagues, tutorials, and help with the globally known Wikidata/Wikibase interface. See 1.1.1 Getting started with Wikidata to get started. The creation of OpenCollection accounts is explained step-by-step in 3.1 Create an Account.\nWhile OpenCollections is almost exclusively built from open-source components and taps into various open knowledge systems, it was not designed to work only with open data; in fact, its primary use is to improve the quality of proprietary, often very confidential, and protected databases. Such databases usually need authoritative data from statistical agencies, scientific bodies, or governmental registries; connecting your systems with open knowledge graphs can ensure that such data is imported without delay and error into your systems. Similarly, you may need to inform the world about your copyright-protected repertoire or the technical details of your product offering. Placing the information on global knowledge graphs ensures that thousands of web services will effortlessly use the updated, new, or correct information about your offering.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "wikidata.html",
    "href": "wikidata.html",
    "title": "1  Wikidata and Other Open Knowledge Graphs",
    "section": "",
    "text": "1.1 Connect to Wikidata\nWikidata is a collaboratively edited multilingual knowledge graph hosted by the Wikimedia Foundation. It is a common source of open data that Wikimedia projects, such as Wikipedia, and anyone else, is able to use under the CC0 public domain license. As of early 2023, Wikidata had 1.54 billion item statements, or small, verifiable, scientific statements about our world.\nWikidata is a document-oriented database, focusing on items, which represent any kind of topic, concept, or object.\nKnowledge graphs connect things in the real world, because their nodes - in Wikidata, the conceptual document -, represent people, objects, and their relationships as they are out there, and not as they are represented by an “ordinary” database . The Q42 document about the late English writer and humorist Douglas Adams connects facts about his life (birthday, place of birth, time of death), and connects him to his books, their translations, identifiers to look up these books, and so on.\nWikidata is a knowledge graph: it connects the concept of Douglas Adams (Q42), to the concept of his most quoted humorous episode from his world-famous Hitchhiker’s Guide to the Galaxy (Q25169) , which is a similarly structured document about the five books of his series, which document is further connected in the graph to the concept of the books’ Serbian translation (Q117279887).\nWikidata is not a database but a very useful system for filling up and keeping many databases in sync worldwide. If your own institutional or private library has a catalogue, you may have a copy of the Hitchhiker’s Guide to the Galaxy; in this case, your catalogue is likely to have a local, private identifier to your copy of the book. Imagine your little private catalogue, where you, like the editors of Wikidata, reserved the #42 entry to Douglas Adams’ book.\nIf you can connect your My-42 entry of Hitchhiker’s Guide to the Galaxy with the books’ Wikidata entry Q25169, you can import a wealth of information into your private catalogue. Furthermore, if you connect the Wikidata item Q42 of the author Douglas Adams to your catalogue’s own entry about the author, you can import a lot of additional knowledge, for example, information about his other works, or the end term of these books’ copyright protection, after which they will become public domain and they will be free to copy and distribute.\nIn Wikidata, each item has a unique, persistent identifier, a positive integer number, prefixed with the upper-case letter Q, known as a “QID”.Global information systems like to anchor authoritative information about people, books, musical works, and other important things to persistent identifiers. For example, in VIAF, the authority file that keeps information synchronised across national libraries, Douglas Adams’ persistent identifier is 113230702, whereas in the Portugese National Library it is 68537. Wikidata is particularly useful because it serves as an “identity broker”, and this linking information can be retrieved directly from Douglas Adams’ Q42 page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wikidata and Other Open Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikidata.html#sec-wikidata",
    "href": "wikidata.html#sec-wikidata",
    "title": "1  Wikidata and Other Open Knowledge Graphs",
    "section": "",
    "text": "Wikidata is a document-oriented database. This document connects a lot of knowledge about the late English writer and humorist, Douglas Adams.\n\n\n\n\n\n\n\n\nID\nAuthor\nTitle\n\n\n\n\nMy-01\nMartell, Yann (Q13914)\nLife of Pi (Q374204)\n\n\n…\n…\n…\n\n\nMy-42\nAdams, Douglas (Q42)\nHitchhiker’s Guide to the Galaxy (Q25169)\n\n\n…\n…\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 Getting started with Wikidata\n\n1.1.1.1 Global Identities\nMr and Mrs Barasits, a.k.a. János Barasits (1859-1935) and his wife, Barasits, Jánosné, born Pichler, Kornélia, were prominent postcard producers and publishers at the beginning of the 20th century. They produced plenty of beautiful postcards.\nIn the 1920s and 1930s, the authors’ right (~copyright) protection of photographs and postcards was relatively short, only 15 years, so their postcards went into the public domain in terms of copying long ago. Plenty of their beautiful works is out there on the internet, but it is very hard to put them into a collection, because most databases know next to nothing about the identities of these creators and their creations.\nUnfortunately, you cannot find their name in the most commonly used authority controls, i.e., VIAF or ISNI. Writing to VIAF is only possible via member institutions, and ISNI costs money. A temporary solution is to create a Wikidata QID for János Barasits (Q124423018), until somebody registers his name into VIAF. With this entry, it will be easier to find further postcards from them, or other information about them all over the world!\nWriting in Wikidata is free for all and subject to community review. If you read this tutorial, please pledge to record new persons (or other items) into Wikidata, only if your knowledge is solid. You can verify the information needed through proper research.\n\n\n1.1.1.2 Create a Wikidata Item\nIn this tutorial, you can learn how to create a new item on Wikidata. Countless web and AI applications and millions of people use Wikidata, so in the beginning it is recommended to not experiment with it in the live system. Wikidata has a Sandbox for practising. We recommend using it as a first step. If you work with Wikibase, particularly with Reprex’s OpenCollections, you will have access to a similar sandbox. It will be prefilled with data, concepts, and properties suitable for your learning needs, often going beyond what you would find in the public Wikidata.\n\nYou can see how creating a new item looks like in the system:\n\n\n\n\n\nThe first step in creating an item (in this case an item for János Barasits) is providing the two most important information for an item, which is the “Label” and the “Description”.\nThe “Label” is the name of the item (in our case the label of the item will be “János Barasits”).\nThe “Description” contains a short explanation of our item (in our case the description for the item will be “Hungarian postcard maker and publisher).\n“Aliases” are alternative names for the item.\n\nAfter creating the item with the basic information of “Label” and “Description” we can weave this information entry into the knowledge graph. At this point, János Barasits could be a person, it could be a book titled after the person, or a photo of the person. Connecting János Barasits to other entities, such as the concept of a human being, will allow other people and their computer systems to understand that we are talking about a person here.You can do that by creating “Statements”. The property “instance of” defines the class our item is a particular example or member of. In this case we would like to make a statement about our item “János Barasits” defining with the property “instance of” that he is a member of the “human” class.\n\n\n\n\n\nThrough the sandbox explore the different type of properties and statements. Add a few, basic statement to your new item: - János Barasits is a human - his gender was male - he was born in 1859 (with the precision of a year only) - he died in 1935\nIt should look similar to this:\n\n\n\n\n\nTo see another example on how useful knowledge graphs can be compared to basic databases consider the following. The four character, 1935 can be understood as a number for most readers, but such a data point without a defined meaning is useless. In a basic database you would see 1935 and know that it is a number. However, in knowledge graphs, like here on Wikidata, when we add the “metadata”, and we connect 1935 to the definition of date of death, we add a meaning (“semantics”) to the number 1935. Now, 1935 is not only a number but also a date of someone’s death.\nThe definition of date of death is useful, but in a knowledge base, we can do ecen more with this piece of information. With this information we can combine the fact that in Europe the copyright protection term of people’s creation runs up to 70 years after their death. Thus, a knowledge base can infere the fact that currently János Barasits’s postcards are out of copyright and they can be freely copied and distributed!\nHere is a very basic Wikidata page for János Barasits. What is very important, is that we have a globally unique identifier, Q124423018 that uniquely identifies him as a human. If you have a collection of postcards (digitals or analogue, vintage physical objects), connecting your own database with Q124423018 will help you to import the knowledge of the expired copyright protection term; it will help you finding other out-of-copyright scanned copies of Barasits’ postcards; it will be easier to connect to other collections that hold items from them, and so on.\n\n\n\n1.1.2 Retrieve an item from Wikidata\nMany internal enterprise resource systems or APIs use SQL, a 50+ year-old data query language. SQL is the lingua franca of relational database systems; you may be familiar with it. Can you query Wikidata in SQL?\nNot exactly. It requires a different querying language, which was developed for knowledge graphs. It is called SPARQL because it is similar to SQL, but they are rather distant cousins.\nWhile SQL is widely used, it does have a significant limitation: your query scripts are specific to one database system or API. What works in your internal catalogue may not function in another organization’s. If you’ve written a script to update your data from a specific web API, it doesn’t guarantee that the script will be compatible with another API. Furthermore, it’s not future-proof: if the API owner (or your database manager) makes even a slight adjustment to the system, you may need to modify or rewrite your retrieval code.\nRemember, the significant advantage of Wikidata and other open knowledge graphs is that millions of people work on improvements and extensions daily. This means that an SQL request would be outdated every day. Instead of SQL, SPARQL queries do not look for cells in data tables, but they use intelligent knowledge to find the cells containing data about what you need. In SQL, you need to know which table contains people’s birthdays and death dates to find out the year when János Barasits died. In SPARQL, you are looking for the cell that contains the date of death for the human known as János Barasits.\n\n\n1.1.3 SPARQL basics\nA simple SPARQL query looks like this:\n\nSELECT ?a ?b ?c\nWHERE\n{\n  x y ?a.\n  m n ?b.\n  ?b f ?c.\n}\n\nSuppose we want to list all children of the baroque composer Johann Sebastian Bach. Using pseudo-elements like in the queries above, how would you write that query?\nHopefully you got something like this:\n\nSELECT ?child\nWHERE\n{\n  #  child \"has parent\" Bach\n  ?child parent Bach.\n  # (note: everything after a ‘#’ is a comment and ignored by WDQS.)\n}\n\nor this,\n\nSELECT ?child\nWHERE\n{\n  # child \"has father\" Bach \n  ?child father Bach. \n}\n\nor this,\n\nSELECT ?child\nWHERE\n{\n  #  Bach \"has child\" child\n  Bach child ?child.\n}\n\nThe first two triples say that the ?child must have the parent/father Bach; the third says that Bach must have the child ?child. Let’s go with the second one for now.\nSo what remains to be done in order to turn this into a proper WDQS query? On Wikidata, items and properties are not identified by human-readable names like “father” (property) or “Bach” (item). (For good reason: “Johann Sebastian Bach” is also the name of a German painter, and “Bach” might also refer to the surname, the French commune, the Mercury crater, etc.) Instead, Wikidata items and properties are assigned an identifier. To find the identifier for an item, we search for the item and copy the Q-number of the result that sounds like it’s the item we’re looking for (based on the description, for example). To find the identifier for a property, we do the same, but search for “P:search term” instead of just “search term”, which limits the search to properties. This tells us that the famous composer Johann Sebastian Bach is Q1339, and the property to designate an item’s father is P:P22.\nAnd last but not least, we need to include prefixes. For simple WDQS triples, items should be prefixed with wd:, and properties with wdt:. (But this only applies to fixed values – variables don’t get a prefix!)\nPutting this together, we arrive at our first proper WDQS query:\n\nSELECT ?child\nWHERE\n{\n# ?child  father   Bach\n  ?child wdt:P22 wd:Q1339.\n}\n\nTry the first query:\n\n\n\nClick on the image to try it out live on the Wikidata SPARQL Endpoint. The query will run if you press the ▶ sign on the endpoint in the bottom left corner.\n\n\nThe first querry will provide you with identifiers, which is great if you are a programmer and you are wiring your database to Wikidata, but less impressive if you are getting familiar with SPARQL and you want to see clearly the fruits of your work.\nLuckily, Wikidata has a human-friendly extension to SPARQL. If you add the following command to your query: SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\".somewhere within the WHERE clause, you get additional variables: For every variable ?foo in your query, you now also have a variable ?fooLabel, which contains the label of the item behind ?foo.\nIf you add this to the SELECT clause, you get the item as well as its label:\n\nSELECT ?child ?childLabel\nWHERE\n{\n# ?child  father   Bach\n  ?child wdt:P22 wd:Q1339.\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n}\n\n\n\n\nClick on the image to try it out live on the Wikidata SPARQL Endpoint. The query will run if you press the ▶ sign on the endpoint in the bottom left corner.\n\n\nTry running that query – you should see not only the item numbers, but also the names of the various children.\n\n\n\nchild\nchildLabel\n\n\n\n\nwd:Q57225\nJohann Christoph Friedrich Bach\n\n\nwd:Q76428\nCarl Philipp Emanuel Bach\n\n\n…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wikidata and Other Open Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "wikibase.html",
    "href": "wikibase.html",
    "title": "2  Wikibase and Enterprise Knowledge Graphs",
    "section": "",
    "text": "2.1 Wikibase\nWikibase is a softver system that help the collaborative management of knowledge in a central repository. It was originally developed for the management of Wikidata, but it is available now for the creation of private, or public-private partnership knowledge graphs. Its primary components are the Wikibase Repository, an extension for storing and managing data, and the Wikibase Client which allows for the retrieval and embedding of structured data from a Wikibase repository. It was developed by Wikimedia Deutschland.\nThe data model for Wikibase links consists of “entities” which include individual “items”, labels or identifier to describe them (potentially in multiple languages), and semantic statements that attribute “properties” to the item. These properties may either be other items within the database, or textual information.\nWikibase has a JavaScript-based user interface, and provides exports of all or subsets of data in many formats. Projects using it include Wikidata, Wikimedia Commons,[5] Europeana’s Eagle Project, Lingua Libre,[6] FactGrid, and the OpenStreetMap wiki.[7]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wikibase and Enterprise Knowledge Graphs</span>"
    ]
  },
  {
    "objectID": "sandbox.html",
    "href": "sandbox.html",
    "title": "3  Reprex’s Sandbox",
    "section": "",
    "text": "3.1 Create an Account\nDepending on the type of MediaWiki+Wikibase instance you are using, you may need to create an account to access the site. The process may be less or more strict, depending on how much private data the instance holds.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "sandbox.html#sec-opencollections-create-account",
    "href": "sandbox.html#sec-opencollections-create-account",
    "title": "3  Reprex’s Sandbox",
    "section": "",
    "text": "Access Reprex’s Sandbox Environment.\nOn this page, select Requess Account.\n\n\n\nOn the next page, type in your chosen username and your email address. For a username, use a professional one that is similar to what you use on Keybase, Github, etc. Then confirm by clicking request account again.\n\n\n\nCheck your email inbox now. You should receive an email with a confirmation link. Click on this confirmation link. (The machine-generated email may easily go to the spam box.)\n\n\n\nAfter you confirm your account request, the administrators of the Wikibase instance will evaluate it. Then, you will receive another email with your login credentials, including your temporary password. \nRevisit the sandbox page and log in. On the login page, type your username and the temporary password you received, then click Log In. You will be automatically taken to the next page, where you must change your password by typing your new permanent password. Provide your new password, then confirm it.\n\n\n\nAll done; you are now logged in to your account.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "sandbox.html#editing-data",
    "href": "sandbox.html#editing-data",
    "title": "3  Reprex’s Sandbox",
    "section": "3.2 Editing data",
    "text": "3.2 Editing data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "sandbox.html#weaving-data-into-the-knowledge-graph",
    "href": "sandbox.html#weaving-data-into-the-knowledge-graph",
    "title": "3  Reprex’s Sandbox",
    "section": "3.3 Weaving Data Into the Knowledge Graph",
    "text": "3.3 Weaving Data Into the Knowledge Graph\nJust because we edit data in a Wikibase instance, it will not necessarily be more usable than a spreadsheet or a simple local database. If we add 42 without a context, such as age or the number of tracks, these two numeric characters will be only literal numbers. We can increase knowledge by making every point of information a node in the knowledge graph, an edge where new information can flow in.\nIn Wikibase, we call these nodes entities. If we make Albert Einstein an entity instead of the string Albert Einstein, we will be able connect knowledge about his life, his scientific work, proofs, photographs of his lectures, and other forms of knowledge.\nWhen we start importing information into a knowledge graph or begin editing and enriching information within the graph, we are faced with a crucial decision. We must determine which data points, such as cells in an original spreadsheet, or database table, or financial ledger, should be elevated to the status of nodes in the graph. These nodes, or entities, have the potential to develop their own relationships, thereby enriching the overall knowledge graph. Understanding this decision-making process empowers us to effectively utilize Wikibase for our data management needs.\n\n3.3.1 Improving relational databases\nWhen the aim is to improve the data quality, content, or timeliness of a relational database system, the first and most essential candidates to become entities are the database’s primary and secondary keys. To recall our simple example from Section 1.1,\n\n\n\nID\nAuthor\nTitle\n\n\n\n\nMy-01\nMartell, Yann (Q13914)\nLife of Pi (Q374204)\n\n\n…\n…\n…\n\n\nMy-42\nAdams, Douglas (Q42)\nHitchhiker’s Guide to the Galaxy Q25169)\n\n\n…\n…\n…\n\n\n\n\nIf you can connect your My-42 entry with Q25169 on Wikidata, you can import a wealth of information into your private catalogue. And if you add Q42 to the author Douglas Adams, you can import a lot of knowledge, for example, information about his other works or the end of the copyright protection term of these books, after which they will become public domain and free for copying and distribution.\n\nIntuitively, in Wikibase, this means that we “conceptualise” authors and their books. The person known as Douglas Adams becomes a human, a creator, and a writer, with all the properties that writers have… such as books. The Hitchhiker’s Guide to the Galaxy will turn from string into the concept of a Book. As soon as we state that this is a book, not merely a text, we can start adding book-specific knowledge to the Hitchhiker's Guide to the Galaxy book entity: ISBN number, first publication date, translations. And what is most important, we can connect this entity with the author, Douglas Adams, who is no longer just one of the many people who are known by this name, but the person who wrote quiet humorous books.\nConceptualisation is possibly manually, as we have shown in Section 1.1; but usually we do this after data modelling with bulk importing. You tells us what is your data about: books and author, and we import them as Books and Authors, so that we can start to look for more information about these books and authors in various knowledge systems.\n\n\n3.3.2 Improving spreadsheet databases\nSmaller organisations often do not use relational databases; instead, they use Excel or OpenOffice spreadsheets maintained by workers, often for decades. Turning such spreadsheets into knowledge base elements is similar to working with a relational database, but sometimes, it is a smaller and more difficult task.\nWell-organised spreadsheets can be good databases because spreadsheet applications like Excel, OpenOffice, or Google Spreadsheet allow the use of primary and secondary keys by connecting worksheets and the creation of pivot tables.\nSmaller organisations often do not use relational databases; instead, they use Excel or OpenOffice spreadsheets maintained by workers, often for decades. Turning such spreadsheets into knowledge base elements is similar to working with a relational database, but sometimes, it is a smaller and more difficult task.\nWell-organised spreadsheets can be good databases because spreadsheet applications like Excel, OpenOffice, or Google Spreadsheet allow the use of primary and secondary keys by connecting worksheets and the creation of pivot tables.\nThe key challenge with spreadsheets is identifying the Things that should become entities. What is your spreadsheet about? Buildings? Then, addresses and building names should become entities and nodes in the graph. Addresses keep changing, building geometries keep changing, and new additions are built or demolished. Street names change. Even city names change; cities merge and divide.\n\n\n3.3.3 Improving annotated text, legal documents, lab notes, regulatory filings\n\n\n3.3.4 Creating new indicators",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reprex's Sandbox</span>"
    ]
  },
  {
    "objectID": "opencollections.html",
    "href": "opencollections.html",
    "title": "4  OpenCollections",
    "section": "",
    "text": "4.1 Going Beyond Wikibase\nOur system is inspired by the WB-CIDOC model developed at the University of Helsinki for translating knowledge stored in Wikibase into the statements described with the CIDOC ontology used by intelligent cultural heritage systems (Kesäniemi, Koho, and Hyvönen 2022). CIDOC is a modern, events-based ontology that allows building trustworthy inference and deduction AI engines.\nThe WB-CIDOC provides rules for writing data into Wikibase in a way that translates correctly into an event-based model, but we find its use counter-intuitive and laborious for domain expert data curators.\nMost domain experts would think that a biographical entity of Albert Einstein should have a birthday property with the date of March 14, 1879, while an event-based ontology would create first an abstract event, the Birth of Albert Einstein, with a timespan of March 14, 1879, 0:00 to 23.59. It is far easier to search for parallel events in this time window or connect further information— like persons present at birth, certificates created, etc.—than to connect this information to a simple, literal date.\nDomain-level experts like copyright specialists, ESG experts, musicologists, bank professionals, and other users usually need formal computer- or information science training and find the entity-based approach closer to real-world experience. We design our knowledge-base instances with hooks for more complex knowledge-base ontologies. This allows our users to review the information in a natural, entity-based format; our intelligent applications translate the information to more complex structures, such as event-based conceptual models, to allow more reasoning capacity for our AI systems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenCollections</span>"
    ]
  },
  {
    "objectID": "opencollections.html#going-beyond-wikibase",
    "href": "opencollections.html#going-beyond-wikibase",
    "title": "4  OpenCollections",
    "section": "",
    "text": "4.1.1 Translation to more complex data models\nOur system is inspired by the WB-CIDOC model developed at the University of Helsinki for translating knowledge stored in Wikibase into the statements described with the CIDOC ontology used by intelligent cultural heritage systems (Kesäniemi, Koho, and Hyvönen 2022). CIDOC is a modern, events-based ontology that allows building trustworthy inference and deduction AI engines.\nThe WB-CIDOC provides rules for writing data into Wikibase in a way that translates correctly into an event-based model, but we find its use counter-intuitive and laborious for domain expert data curators.\nMost domain experts would think that a biographical entity of Albert Einstein should have a birthday property with the date of March 14, 1879, while an event-based ontology would create first an abstract event, the Birth of Albert Einstein, with a timespan of March 14, 1879, 0:00 to 23.59. It is far easier to search for parallel events in this time window or connect further information— like persons present at birth, certificates created, etc.—than to connect this information to a simple, literal date.\n\n\n\n\n\n\nNote\n\n\n\nDomain-level experts like copyright specialists, ESG experts, musicologists, bank professionals, and other users usually need formal computer- or information science training and find the entity-based approach closer to real-world experience. We design our knowledge-base instances with hooks for more complex knowledge-base ontologies. This allows our users to review the information in a natural, entity-based format; our intelligent applications translate the information to more complex structures, such as event-based conceptual models, to allow more reasoning capacity for our AI systems.\n\n\n\n\n4.1.2 Record-keeping and retention\nNational archives play a crucial role in preserving the collective memory and history of a nation. Connecting national archives to institutional enterprise record-keeping systems has many advantages.\n\nContextualising institutional or enterprise records: Private organisations and users cannot copy all legally or historically relevant documents in their inventory. Connecting to memory institutions, such as records or legal databases, allows one to find precedents and understand one and one’s own historical records in context without the need to hoard information on an excessive scale. Just the way we do not need to burden our office bookshelves with bilingual dictionaries or printed copies of changing regulations, we can further lower the burden by making our records system compatible with national records.\nRecord retention and public archiving is a regulated process that serves as the foundation of many business processes’ regulatory or assurance oversight. Businesses often must deposit copies of legally important disclosures and certificates at public bodies. Larger institutions, primarily if they work for the public benefit, usually have a legal mandate to place some of their documents into a public archive. Private persons and companies often donate documents to such archives when they want to be credited with their work, intellectual property, or the value of their activities.\n\nBecause OpenCollections is based around a document-based database, it is very well suited to support document exchanges between private institutions (e.g., the exchange of technical and delivery documentation along the supply chain), public institutions (e.g., the exchange of public documents), and public-private exchanges.\nWe provide mappings, software tools and training to apply Records in Context (RiC), a novel ontology released in 2023 after over a decade’s work to replace the four international standards on archiving. The last international standards on the topic were created before the commercial Internet; RiC provides backwards compatibility to millennia of historical records, corporate document inventories, and physical data vaults on one hand, and opens up the use of modern knowledge graphs to link information in the archives with your documents in use. RiC is the gateway to corporate and institutional textual big data.\n\n\n4.1.3 Data catalogues, and the meaning of data tables\nFollowing the DCAT-AP specification of the EU Open Data Portal and Stat-DCAT-AP to offer full compatibility with European statistical portals and open data portals, we translate information about datasets, data codes and structures, and variable descriptions. This translation works with few limitations for global resources beyond Europe. It connects corporate or institutional datasets and accounts with statistical and national accounts data from public sources, offering unparalleled ease in creating economic or sustainability-controlling applications.\n\n\n4.1.4 Collections and inventories\n\n\n\n\nKesäniemi, Joonas, Mikko Koho, and Eero Hyvönen. 2022. “Using Wikibase for Managing Cultural Heritage Linked Open Data Based on CIDOC CRM.” In New Trends in Database and Information Systems, edited by Silvia Chiusano, Tania Cerquitelli, Robert Wrembel, Kjetil Nørvåg, Barbara Catania, Genoveva Vargas-Solar, and Ester Zumpano, 542–49. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-15743-1_49.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenCollections</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Kesäniemi, Joonas, Mikko Koho, and Eero Hyvönen. 2022. “Using\nWikibase for Managing Cultural Heritage Linked Open Data Based on\nCIDOC CRM.” In New Trends in\nDatabase and Information Systems, edited by Silvia Chiusano, Tania\nCerquitelli, Robert Wrembel, Kjetil Nørvåg, Barbara Catania, Genoveva\nVargas-Solar, and Ester Zumpano, 542–49. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-031-15743-1_49.",
    "crumbs": [
      "References"
    ]
  }
]